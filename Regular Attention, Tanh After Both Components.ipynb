{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Packages and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:27:08.067623Z",
     "start_time": "2018-05-26T10:27:06.715372Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import pylab as plt\n",
    "\n",
    "import nltk\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Image as DisplayImage\n",
    "\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tokens import Doc\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import os\n",
    "# GPU Selection\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:28:16.230012Z",
     "start_time": "2018-05-26T10:28:13.018307Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/raw/splits/train_split.json\", \"r\") as f:\n",
    "    train_split = json.load(f)\n",
    "with open(\"data/raw/splits/dev_split.json\", \"r\") as f:\n",
    "    dev_split = json.load(f)\n",
    "with open(\"data/raw/splits/test_split.json\", \"r\") as f:\n",
    "    test_split = json.load(f)\n",
    "    \n",
    "with open(\"data/raw/paragraphs_topics_v1.pickle\", \"rb\") as f:\n",
    "    paragraph_topics = pickle.load(f)\n",
    "    \n",
    "with open(\"data/raw/paragraphs_v1.json\", \"r\") as f:\n",
    "    paragraphs_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-26T10:28:20.311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b30cd6510f4eb891d009117a29e10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19561), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def parse_data(paragraphs_json, progress_bar=True):\n",
    "    train_data = []\n",
    "    dev_data = []\n",
    "    test_data = []\n",
    "    image_ids = {}\n",
    "    topic_set = set()\n",
    "    for i, json in enumerate(tqdm_notebook(paragraphs_json) if progress_bar else paragraphs_json):\n",
    "        topic_to_seq = {}\n",
    "        for j, sentence in enumerate(sent_detector.tokenize(json['paragraph'])):\n",
    "            sentence = sentence.strip().lower()\n",
    "            t = nlp(sentence)\n",
    "            image_id = json['image_id']\n",
    "            image_ids[image_id] = i\n",
    "            \n",
    "            if 'perfect_match' in paragraph_topics[i][j]:\n",
    "                topic_list = set([topic[0] for topic in paragraph_topics[i][j]['perfect_match']])\n",
    "                for topic in topic_list:\n",
    "                    topic_set.add(topic)\n",
    "                    if topic not in topic_to_seq:\n",
    "                        topic_to_seq[topic] = []\n",
    "                    topic_to_seq[topic].extend(t)\n",
    "            for topic in topic_to_seq:\n",
    "                if image_id in train_split:\n",
    "                    train_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "                elif image_id in dev_split:\n",
    "                    dev_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "                elif image_id in test_split:\n",
    "                    test_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "    return train_data, dev_data, test_data, image_ids, topic_set\n",
    "                        \n",
    "train_data, dev_data, test_data, image_ids, topic_set = parse_data(paragraphs_json)\n",
    "print(\"Length of train split: %d\" %len(train_data))\n",
    "print(\"Length of dev split: %d\" %len(dev_data))\n",
    "print(\"Length of test split: %d\" %len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paragraphs_json[1])\n",
    "print(image_ids[(train_data[3][0])])\n",
    "print(train_data[3])\n",
    "print(topic_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"A vocabulary wrapper, contains a word_to_index dictionary and a index_to_word list\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = []\n",
    "        self.index = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word_to_index:\n",
    "            self.word_to_index[word] = self.index\n",
    "            self.index_to_word.append(word)\n",
    "            self.index += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if type(word) == str:\n",
    "            if not word in self.word_to_index:\n",
    "                return self.word_to_index['<UNK>']\n",
    "            return self.word_to_index[word]\n",
    "        else:\n",
    "            return self.index_to_word[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.index\n",
    "\n",
    "def build_vocab(sentences, min_occurrences):\n",
    "    \"\"\"Builds a Vocabulary object\"\"\"\n",
    "    counter = Counter()\n",
    "    for sentence in tqdm_notebook(sentences):\n",
    "        for word in sentence:\n",
    "            counter[word.text] += 1\n",
    "\n",
    "    # a word must appear at least min_occurrence times to be included in the vocabulary\n",
    "    words = [word for word, count in counter.items() if count >= min_occurrences]\n",
    "\n",
    "    # Creating a vocabulary object\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<SOS>')\n",
    "    vocab.add_word('<EOS>')\n",
    "    vocab.add_word('<UNK>')\n",
    "\n",
    "    # Adds the words from the captions to the vocabulary\n",
    "    for word in words:\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "word_vocab = build_vocab([val[2] for val in train_data], 5)\n",
    "topic_vocab = Vocabulary()\n",
    "for topic in topic_set:\n",
    "    topic_vocab.add_word(topic)\n",
    "print(\"Length of word vocab: %d\" %len(word_vocab))\n",
    "print(\"Number of topics: %d\" %len(topic_vocab))\n",
    "print(topic_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the Data\n",
    "The data is batched so sentences of the same length are grouped together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchedData(object):\n",
    "    def __init__(self, batch_size):\n",
    "        self.batched_data = []\n",
    "        self.index = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add_batch(self, batch):\n",
    "        if len(batch) == self.batch_size:\n",
    "            self.batched_data.append(batch)\n",
    "        else:\n",
    "            print(\"not the correct size batch!\")\n",
    "\n",
    "    def __call__(self, index):\n",
    "        if not index < len(self.batched_data):\n",
    "            return []\n",
    "        return self.batched_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batched_data)\n",
    "    \n",
    "def batch_data(data, batch_size, max_size=None, progress_bar=True, randomize=True):\n",
    "    batched_set = {}\n",
    "    counter = 0\n",
    "    for (image_id, topic, sentence) in (tqdm_notebook(data) if progress_bar else data):\n",
    "        # accounting for SOS and EOS tokens\n",
    "        sentence = [token.text for token in sentence]\n",
    "        caption_len = len(sentence) + 2\n",
    "        if caption_len not in batched_set.keys():\n",
    "            batched_set[caption_len] = []\n",
    "        batched_set[caption_len].append((image_id, topic, [token for token in sentence]))\n",
    "\n",
    "    batched_data = BatchedData(batch_size)\n",
    "    curr_size = 0\n",
    "\n",
    "    for i in batched_set.keys():\n",
    "        if len(batched_set[i]) >= batch_size:\n",
    "            batch = batched_set[i]\n",
    "            random.shuffle(batch)\n",
    "        for j in range(len(batch) // batch_size):\n",
    "            if max_size is None or curr_size < max_size:\n",
    "                batched_data.add_batch(batch[batch_size * j : batch_size * (j+1)])\n",
    "                curr_size += 1\n",
    "    if randomize:\n",
    "        random.shuffle(batched_data.batched_data)\n",
    "    return batched_data\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "batched_train_data = batch_data(train_data, batch_size, progress_bar=False)\n",
    "batched_dev_data = batch_data(dev_data, batch_size, progress_bar=False)\n",
    "batched_single_dev_data = batch_data(dev_data, 1, progress_bar=False, randomize=False)\n",
    "batched_test_data = batch_data(test_data, 1, progress_bar=False, randomize=False)\n",
    "\n",
    "print(\"number of train batches: %d\" %len(batched_train_data))\n",
    "print(\"number of dev batches: %d\" %len(batched_dev_data))\n",
    "print(\"number of test batches: %d\" %len(batched_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transform\n",
    "The transformation that will be applied to all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(data.Dataset):\n",
    "    def __init__(self, paragraphs, batched_captions, word_vocab, topic_vocab, transform=None):\n",
    "        \"\"\"\n",
    "        Set the path for images, captions and vocabulary wrapper.\n",
    "    \n",
    "        Args:\n",
    "        \n",
    "                vocab: vocabulary wrapper.\n",
    "                transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.paragraphs = paragraphs\n",
    "        self.batched_captions = batched_captions\n",
    "        self.word_vocab = word_vocab\n",
    "        self.topic_vocab = topic_vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        images = []\n",
    "        topics = []\n",
    "        captions = []\n",
    "        img_ids = []\n",
    "        for (image_id, topic, sentence) in self.batched_captions(index):\n",
    "            image = Image.open(\"../data/\" + str(image_id) + \".jpg\").convert('RGB')\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "            images.append(image)\n",
    "            topics.append(self.topic_vocab(topic))\n",
    "            img_ids.append(image_id)\n",
    "            captions.append([self.word_vocab('<SOS>')] + [self.word_vocab(token) for token in sentence] + [self.word_vocab('<EOS>')])\n",
    "\n",
    "        lengths = [len(caption) for caption in captions]\n",
    "        return torch.stack(images, 0), torch.LongTensor(topics), torch.LongTensor(captions), lengths, img_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batched_captions)\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "  \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging captions (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    images, topics, captions, lengths, image_ids = zip(*data)\n",
    "    return images[0], topics[0], captions[0], lengths[0], image_ids[0]\n",
    "\n",
    "\n",
    "def get_loader(paragraphs, batched_data, word_vocab, topic_vocab, transform, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    data_set = CustomDataSet(paragraphs, batched_data, word_vocab, topic_vocab, transform)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=data_set, \n",
    "                                            shuffle=shuffle,\n",
    "                                            num_workers=num_workers,\n",
    "                                            collate_fn=collate_fn)\n",
    "    return data_loader\n",
    "\n",
    "single_val_data_loader = get_loader(dev_data, batched_single_dev_data, word_vocab, topic_vocab, transform, False, 2)\n",
    "test_data_loader = get_loader(test_data, batched_test_data, word_vocab, topic_vocab, transform, False, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model\n",
    "## The Encoder\n",
    "The encoder is a CNN, specifically VGG-16 with which we strip off the final few layers to get 196 visual feature vectors of size 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        vgg = models.vgg16(pretrained=True).eval()\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = nn.Sequential(*(vgg.features[i] for i in range(29)))\n",
    "    \n",
    "    def forward(self, images):\n",
    "        features = self.vgg(images)\n",
    "        features_reshaped = features.view(-1, 512, 196)\n",
    "        features_transposed = features_reshaped.transpose(1, 2)\n",
    "        return features_transposed\n",
    "    \n",
    "encoder_cnn = EncoderCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder\n",
    "The decoder is an LSTM with attention and dropout. We compute the initial hidden state and cell state as follows:\n",
    "$$h_{0} = \\tanh((\\sum_{i=0}^{196} \\frac{1}{196} f_{initH}(a_{i})) + g_{initH}(t_{j}))$$\n",
    "$$c_{0} = \\tanh((\\sum_{i=0}^{196} \\frac{1}{196} f_{initC}(a_{i})) + g_{initC}(t_{j}))$$\n",
    "where $f_{initH}, f_{initC}, g_{initH}, g_{initC}$ are all linear functions, $a_{i}$ is the $i^{th}$ annotation vector and $t_{j}$ is the topic embedding.\n",
    "We compute attention using the soft attention model from Show Tell Attend as follows:\n",
    "$$e_{ti} = w_{1}(a_{i} + w_{2} h_{t-1}$$\n",
    "$$\\alpha_{ti} = \\frac{\\exp(e_{ti})}{\\sum_{k=1}^{L} \\exp(e_{tk})}$$\n",
    "and the context vector $\\hat{z_{t}} = \\sum_{i=1}^{L} \\alpha_{ti} a_{i}$,\n",
    "where $w_{1}, w_{2}$ are learned matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predict_input_captions(captions):\n",
    "    if torch.cuda.is_available():\n",
    "        return Variable(torch.cuda.LongTensor(captions))\n",
    "    return Variable(torch.LongTensor(captions))\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vis_dim: The size of each visual feature vector\n",
    "        vis_num: The number of visual feature vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, vis_dim, vis_num, embed_dim, hidden_dim, word_vocab_size, topic_vocab_size, num_layers=1, dropout=0.0):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.vis_dim = vis_dim\n",
    "        self.vis_num = vis_num\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        self.topic_vocab_size = topic_vocab_size\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.init_h_vis = nn.Linear(vis_dim, hidden_dim, bias=False)\n",
    "        self.init_c_vis = nn.Linear(vis_dim, hidden_dim, bias=False)\n",
    "        self.init_h_topic = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.init_c_topics = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.attn_vw = nn.Linear(vis_dim, 1)\n",
    "        self.attn_hw = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.topic_embed = nn.Embedding(topic_vocab_size, embed_dim)\n",
    "        self.word_embed = nn.Embedding(word_vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(vis_dim + embed_dim, hidden_dim, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_dim, word_vocab_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def _init_hidden(self, features, topics):\n",
    "        hidden = (torch.sum(self.init_h_vis(features), 1) / self.vis_num) + self.init_h_topic(topics)\n",
    "        cell = (torch.sum(self.init_c_vis(features), 1) / self.vis_num) + self.init_c_topics(topics)\n",
    "        return hidden.unsqueeze(0), cell.unsqueeze(0)\n",
    "    \n",
    "    def _compute_attention(self, features, hidden_state):\n",
    "        \"\"\"\n",
    "        features: B x vis_num x vis_dim\n",
    "        hidden_state: (1 x B x hidden_size, 1 x B x hidden_size)\n",
    "        \"\"\"\n",
    "        # add in L1 norm (sum up everything and divide everything by sum\n",
    "        #features = torch.norm(features, 1, 2, )\n",
    "        # B x vis_num x 1\n",
    "        att_vw = self.attn_vw(features)\n",
    "        # B x vis_num x 1\n",
    "        att_hw = self.attn_hw(hidden_state.transpose(0, 1).repeat(1, self.vis_num, 1))\n",
    "        # B x vis_num x 1\n",
    "        attention = att_vw + att_hw\n",
    "        attention_softmax = F.softmax(attention, dim=1)\n",
    "        # B x vis_dim\n",
    "        # also return the attention_softmax and average those\n",
    "        return torch.sum(features * attention_softmax, 1), attention_softmax\n",
    "\n",
    "    def forward(self, features, topics, captions):\n",
    "        \"\"\"\n",
    "        topic: B x 1\n",
    "        features: B x vis_num x vis_dim\n",
    "        captions: B x seq_length\n",
    "        \"\"\"\n",
    "        topic_embeddings = self.topic_embed(topics)\n",
    "        hidden = self._init_hidden(features, topic_embeddings)\n",
    "        word_embeddings = self.word_embed(captions)\n",
    "        word_space = None\n",
    "        lengths = len(captions[0])\n",
    "        average_attention = None\n",
    "        for i in range(lengths):\n",
    "            embedding = torch.index_select(word_embeddings, 1, Variable(torch.cuda.LongTensor([i])))\n",
    "            attention, alpha = self._compute_attention(features, hidden[0])\n",
    "            attention = attention.unsqueeze(1)\n",
    "            average_attention = alpha if average_attention is None else average_attention + alpha\n",
    "            input = self.dropout(torch.cat([attention, embedding], 2))\n",
    "            out, hidden = self.lstm(input, hidden)\n",
    "            words = self.output(self.dropout(out))\n",
    "            word_space = torch.cat([word_space, words], 1) if word_space is not None else words\n",
    "        word_space = pack_padded_sequence(word_space, [lengths for i in range(len(captions))], batch_first=True)[0]\n",
    "        return F.log_softmax(word_space, dim=1), F.softmax(word_space, dim=1), average_attention\n",
    "    \n",
    "    def sample(self, features, topics, beam_size=1, start_token=0, end_token=1):\n",
    "        topic_embeddings = self.topic_embed(topics)\n",
    "        hidden = self._init_hidden(features, topic_embeddings)\n",
    "        completed_phrases = []\n",
    "        best_phrases = []\n",
    "        score = 0\n",
    "\n",
    "        initial_caption = create_predict_input_captions([start_token])\n",
    "        embedding = self.word_embed(initial_caption)\n",
    "        attention = self._compute_attention(features, hidden[0])\n",
    "        input = torch.cat([attention, embedding], 1).unsqueeze(1)\n",
    "        out, hidden = self.lstm(input, hidden)\n",
    "        words = self.output(out)\n",
    "        word_scores = F.softmax(words, dim=2)\n",
    "        top_scores, top_captions = word_scores.topk(beam_size)\n",
    "        best_phrases = [[top_scores[0][0].data[i], [top_captions[0][0].data[i]]] for i in range(beam_size)]\n",
    "        next_captions = top_captions.resize(beam_size, 1)\n",
    "        hidden = (hidden[0].repeat(1, beam_size, 1), hidden[1].repeat(1, beam_size, 1))\n",
    "\n",
    "        for index in range(30):\n",
    "            best_candidates = []\n",
    "            embedding = self.word_embed(next_captions)\n",
    "            attention = self._compute_attention(features, hidden[0]).unsqueeze(1)\n",
    "            input = torch.cat([attention, embedding], 2)\n",
    "            out, hidden = self.lstm(input, hidden)\n",
    "            words = self.output(out)\n",
    "            word_scores = F.softmax(words, dim=2)\n",
    "            top_scores, top_captions = word_scores.topk(beam_size)\n",
    "            len_phrases = len(best_phrases[0][1])\n",
    "            for i in range(len(best_phrases)):\n",
    "                for j in range(beam_size):\n",
    "                    best_candidates.extend([[best_phrases[i][0] + top_scores[i][0].data[j],\n",
    "                        best_phrases[i][1] + [top_captions[i][0].data[j]],\n",
    "                        i]])\n",
    "            top_candidates = sorted(best_candidates, key=lambda score_caption: score_caption[0])[-beam_size:]\n",
    "            temp_candidates = []\n",
    "            for phrase in top_candidates:\n",
    "                if phrase[1][-1] == end_token:\n",
    "                    completed_phrases.append([phrase[0] / len(phrase[1]), phrase[1]])\n",
    "                else:\n",
    "                    temp_candidates.append(phrase)\n",
    "            top_candidates = temp_candidates\n",
    "            if len(completed_phrases) >= beam_size:\n",
    "                return sorted(completed_phrases, key=lambda score_caption: score_caption[0], reverse=True)[:beam_size]\n",
    "            best_phrases = [[phrase[0], phrase[1]] for phrase in top_candidates]\n",
    "            next_captions = create_predict_input_captions([[phrase[1][-1]] for phrase in top_candidates])\n",
    "            hidden_0 = (torch.stack([hidden[0][0].select(0, phrase[2]) for phrase in top_candidates]).unsqueeze(0))\n",
    "            hidden_1 = (torch.stack([hidden[1][0].select(0, phrase[2]) for phrase in top_candidates]).unsqueeze(0))\n",
    "            hidden = (hidden_0, hidden_1)\n",
    "        return sorted(completed_phrases, key=lambda score_caption: score_caption[0], reverse=True)[:beam_size]\n",
    "\n",
    "decoder_rnn = DecoderRNN(512, 196, 512, 512, len(word_vocab), len(topic_vocab), num_layers=1, dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    encoder_cnn.cuda()\n",
    "    decoder_rnn.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Loss Function and  Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "params = list(decoder_rnn.parameters())\n",
    "optimizer = optim.Adam(params, lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Saved Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint = None\n",
    "if load_checkpoint is not None:\n",
    "    checkpoint = torch.load(load_checkpoint)\n",
    "    print(\"loading from checkpoint \" + str(load_checkpoint))\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    decoder_rnn.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    checkpoint = None\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x, useCuda=True, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "def evaluate(images, topics, captions, encoder_cnn, decoder_rnn, loss_function, volatile=False):\n",
    "    images = to_var(images, volatile)\n",
    "    features = encoder_cnn(images)\n",
    "    topics = to_var(topics, volatile)\n",
    "    inputs = to_var(captions, volatile)[:, :-1]\n",
    "    targets = to_var(captions[:, 1:], volatile)\n",
    "    len_targets = len(targets[0])\n",
    "    targets = pack_padded_sequence(targets, [len_targets for i in range(len(captions))], batch_first=True)[0]\n",
    "    predictions, _, average_attention = decoder_rnn(features, topics, inputs)\n",
    "    loss = loss_function(predictions, targets)\n",
    "    return loss, average_attention.squeeze(0).squeeze(1).data\n",
    "\n",
    "def train(images, topics, captions, encoder_cnn, decoder_rnn, loss_function, optimizer, grad_clip):\n",
    "    decoder_rnn.train()\n",
    "    decoder_rnn.zero_grad()\n",
    "    loss, _ = evaluate(images, topics, captions, encoder_cnn, decoder_rnn, loss_function)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm(decoder_rnn.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    return loss.data.select(0, 0)\n",
    "\n",
    "def val(images, topics, captions, encoder_cnn, decoder_rnn, loss_function, volatile=True):\n",
    "    decoder_rnn.eval()\n",
    "    loss, average_attention = evaluate(images, topics, captions, encoder_cnn, decoder_rnn, loss_function, volatile=volatile)\n",
    "    return loss.data.select(0, 0), average_attention\n",
    "\n",
    "def caption_id_to_string(caption, vocab):\n",
    "    output = \"\"\n",
    "    for word in caption:\n",
    "        if \".\" != word and \"<EOS>\" != word:\n",
    "            output += vocab(word) + \" \"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_data = {}\n",
    "output_val_data = {}\n",
    "attention_val_data = {}\n",
    "\n",
    "if load_checkpoint is not None:\n",
    "    with open(\"train_\" + str(start_epoch) + \".pkl\", \"rb\") as f:\n",
    "        output_train_data = pickle.load(f)\n",
    "    with open(\"val_\" + str(start_epoch) + \".pkl\", \"rb\") as f:\n",
    "        output_val_data = pickle.load(f)\n",
    "    with open(\"attention_\" + str(start_epoch) + \".pkl\", \"rb\") as f:\n",
    "        attention_val_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "grad_clip = 5.0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # Reshuffle data\n",
    "    batched_train_data = batch_data(train_data, batch_size, progress_bar=False)\n",
    "    batched_dev_data = batch_data(dev_data, batch_size, progress_bar=False)\n",
    "    train_data_loader = get_loader(train_data, batched_train_data, word_vocab, topic_vocab, transform, True, 2)\n",
    "    val_data_loader = get_loader(dev_data, batched_dev_data, word_vocab, topic_vocab, transform, True, 2)\n",
    "    temp_val_loader = get_loader(dev_data, batched_dev_data, word_vocab, topic_vocab, transform, True, 2)\n",
    "\n",
    "    progress_bar = tqdm_notebook(iterable=train_data_loader, desc='Epoch [%i/%i] (Train)' %(epoch + 1 , num_epochs))\n",
    "    train_sum_loss = 0.0\n",
    "    \n",
    "    for i, (images, topics, captions, lengths, ids) in enumerate(progress_bar):\n",
    "        loss = train(images, topics, captions, encoder_cnn, decoder_rnn, loss_function, optimizer, grad_clip)\n",
    "        train_sum_loss += loss\n",
    "        progress_bar.set_postfix(loss=train_sum_loss/((i % 100) + 1))\n",
    "        if (i + 1) % 100 == 0:\n",
    "            output_train_data[epoch * len(train_data_loader) + i + 1] = train_sum_loss / 100\n",
    "            train_sum_loss = 0\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            temp_val_loss = 0.0\n",
    "            for j, (images, topics, captions, lengths, ids) in enumerate(temp_val_loader):\n",
    "                if j == 100:\n",
    "                    break\n",
    "                temp_val_loss += val(images, topics, captions, encoder_cnn, decoder_rnn, loss_function)[0]\n",
    "            output_val_data[epoch * len(train_data_loader) + i + 1] = temp_val_loss / 100\n",
    "    \n",
    "    # end of batch\n",
    "    output_train_data[(epoch + 1) * len(train_data_loader)] = train_sum_loss / (len(train_data_loader) % 100)\n",
    "    val_sum_loss = 0\n",
    "    \n",
    "    val_progress_bar = tqdm_notebook(iterable=val_data_loader, desc='Epoch [%i/%i] (Val)' %(epoch + 1, num_epochs))\n",
    "    for i, (images, topics, captions, lengths, ids) in enumerate(val_progress_bar, 1):\n",
    "        loss, average_attention = val(images, topics, captions, encoder_cnn, decoder_rnn, loss_function)\n",
    "        attention_val_data[(epoch + 1, ids[0])] = average_attention.tolist()\n",
    "        val_sum_loss += loss\n",
    "        val_progress_bar.set_postfix(loss=val_sum_loss/i)\n",
    "    output_val_data[(epoch + 1) * len(train_data_loader)] = val_sum_loss / len(val_data_loader)\n",
    "    \"\"\"\n",
    "    if (epoch % 5 == 0 and epoch != 0):\n",
    "        generate_progress_bar = tqdm_notebook(iterable=single_val_data_loader)\n",
    "        with open(\"captions_\" + str(epoch) + \".txt\", \"w\") as f:\n",
    "            f.write(\"[\")\n",
    "            for i, (images, topics, captions, lengths, ids) in enumerate(generate_progress_bar, 1):\n",
    "                images = to_var(images)\n",
    "                features = encoder_cnn(images)\n",
    "                topics = to_var(topics)\n",
    "                results = decoder_rnn.sample(features, topics, beam_size)\n",
    "                results_json = json.dumps({'image_id': ids[0], 'caption': caption_id_to_string(results[0][1], word_vocab)})\n",
    "                f.write(results_json)\n",
    "                f.write(\",\")\n",
    "            f.write(\"]\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trial_number = sorted(output_train_data.keys())\n",
    "train_trial_loss = [output_train_data[i] for i in train_trial_number]\n",
    "val_trial_number = sorted(output_val_data.keys())\n",
    "val_trial_loss = [output_val_data[i] for i in val_trial_number]\n",
    "\n",
    "\n",
    "plt.plot(train_trial_number, train_trial_loss, 'ro', markersize=1, label='Training Batch Loss')\n",
    "plt.plot(val_trial_number, val_trial_loss, 'bo', markersize=5, label='Validation Loss After 1 Full Epoch')\n",
    "\n",
    "plt.xlabel('Number of Batches')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over ' + str(num_epochs) + ' Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 10\n",
    "batched_temp_data = batch_data(train_data, 1, progress_bar=False)\n",
    "temp_data_loader = get_loader(train_data, batched_temp_data, word_vocab, topic_vocab, transform, True, 2)\n",
    "for images, topics, captions, lengths, ids in temp_data_loader:\n",
    "    json = paragraphs_json[image_ids[ids[0]]]\n",
    "    DisplayImage(json['url'])\n",
    "    print(json['url'])\n",
    "    print(\"actual caption is: \")\n",
    "    for sentence in sent_detector.tokenize(json['paragraph']):\n",
    "        print(sentence.strip().lower())\n",
    "        \n",
    "    images = to_var(images)\n",
    "    features = encoder_cnn(images)\n",
    "    topics = to_var(topics)\n",
    "    results = decoder_rnn.sample(features, topics, beam_size)\n",
    "    print(\"predicted captions for topic %s: \" %(topic_vocab(topics.data.select(0, 0))))\n",
    "    for result in results:\n",
    "        candidate = [word_vocab(i) for i in result[1][:-1]]\n",
    "        print(candidate)\n",
    "    random_topic = torch.LongTensor([random.randint(0, len(topic_vocab) - 1)])\n",
    "    random_topic = to_var(random_topic)\n",
    "    print(\"testing changing topic to %s:\" %(topic_vocab(random_topic.data.select(0, 0))))\n",
    "    results = decoder_rnn.sample(features, random_topic, beam_size)\n",
    "    for result in results:\n",
    "        candidate = [word_vocab(i) for i in result[1][:-1]]\n",
    "        print(candidate)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'epoch': num_epochs,\n",
    "            'state_dict': decoder_rnn.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()},\n",
    "            \"checkpoints/checkpoint_\" + str(num_epochs) + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_\" + str(num_epochs) + \".pkl\", \"wb\") as f:\n",
    "    pickle.dump(output_train_data, f)\n",
    "with open(\"val_\" + str(num_epochs) + \".pkl\", \"wb\") as f:\n",
    "    pickle.dump(output_val_data, f)\n",
    "with open(\"attention_\" + str(num_epochs) + \".pkl\", \"wb\") as f:\n",
    "    pickle.dump(attention_val_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Custom Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"snow\"\n",
    "topic = torch.LongTensor([10])\n",
    "image = Image.open(\"../data/custom/\" + image_name + \".jpg\").convert('RGB')\n",
    "images = [transform(image)]\n",
    "images = torch.stack(images, 0)\n",
    "images = to_var(images)\n",
    "features = encoder_cnn(images)\n",
    "topics = to_var(topic)\n",
    "results = decoder_rnn.sample(features, topics, 10)\n",
    "print(\"predicted captions for topic %s: \" %(topic_vocab(topics.data.select(0, 0))))\n",
    "for result in results:\n",
    "    candidate = [word_vocab(i) for i in result[1][:-1]]\n",
    "    print(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 2331872\n",
    "# 2346802 is a snow image\n",
    "epoch = 2\n",
    "alpha_values = attention_val_data[(epoch, image_id)]\n",
    "max_alpha = max(alpha_values)\n",
    "alpha_values = [alpha / max_alpha for alpha in alpha_values]\n",
    "\n",
    "original_image = Image.open(\"../data/\" + str(image_id) + \".jpg\") \\\n",
    "    .convert('L').resize((256, 256), Image.ANTIALIAS).crop((0, 0, 224, 224))\n",
    "\n",
    "image = Image.open(\"../data/\" + str(image_id) + \".jpg\").convert('L')\n",
    "image = image.resize((256, 256), Image.ANTIALIAS)\n",
    "image = image.crop((0, 0, 224, 224))\n",
    "pixels = image.load()\n",
    "row_pixels = image.size[0] // 14\n",
    "col_pixels = image.size[1] // 14\n",
    "for i in range(image.size[0]):    # for every col:\n",
    "    for j in range(image.size[1]):    # For every row\n",
    "        alpha = alpha_values[(i // row_pixels) * 14 + j // col_pixels]\n",
    "        pixels[i, j] = int(alpha * pixels[i, j])\n",
    "result = Image.blend(image, original_image, 0.2)\n",
    "result = result.resize((448, 448), Image.ANTIALIAS)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
