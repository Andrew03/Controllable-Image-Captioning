{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request \n",
    "for json in tqdm_notebook(paragraphs):\n",
    "    urllib.request.urlretrieve(json['url'], \"data/\" + str(json['image_id']) + \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Packages and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "import pylab as plt\n",
    "\n",
    "import nltk\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Image as DisplayImage\n",
    "\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tokens import Doc\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import progressbar\n",
    "\n",
    "import os\n",
    "# GPU Selection\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from nets.vgg16 import vgg16\n",
    "from nets.resnet_v1 import resnetv1\n",
    "from model.test import im_detect\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for vgg16:\n\tWhile copying the parameter named \"rpn_cls_score_net.weight\", whose dimensions in the model are torch.Size([18, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([24, 512, 1, 1]).\n\tWhile copying the parameter named \"rpn_cls_score_net.bias\", whose dimensions in the model are torch.Size([18]) and whose dimensions in the checkpoint are torch.Size([24]).\n\tWhile copying the parameter named \"rpn_bbox_pred_net.weight\", whose dimensions in the model are torch.Size([36, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([48, 512, 1, 1]).\n\tWhile copying the parameter named \"rpn_bbox_pred_net.bias\", whose dimensions in the model are torch.Size([36]) and whose dimensions in the checkpoint are torch.Size([48]).\n\tWhile copying the parameter named \"cls_score_net.weight\", whose dimensions in the model are torch.Size([21, 4096]) and whose dimensions in the checkpoint are torch.Size([81, 4096]).\n\tWhile copying the parameter named \"cls_score_net.bias\", whose dimensions in the model are torch.Size([21]) and whose dimensions in the checkpoint are torch.Size([81]).\n\tWhile copying the parameter named \"bbox_pred_net.weight\", whose dimensions in the model are torch.Size([84, 4096]) and whose dimensions in the checkpoint are torch.Size([324, 4096]).\n\tWhile copying the parameter named \"bbox_pred_net.bias\", whose dimensions in the model are torch.Size([84]) and whose dimensions in the checkpoint are torch.Size([324]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cd8c9a6e2d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#net = resnetv1(num_layers=152)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_scales\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vgg16_faster_rcnn_iter_1190000.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Controllable-Image-Captioning/lib/nets/network.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0mTo\u001b[0m \u001b[0mprovide\u001b[0m \u001b[0mback\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \"\"\"\n\u001b[0;32m--> 489\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 721\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for vgg16:\n\tWhile copying the parameter named \"rpn_cls_score_net.weight\", whose dimensions in the model are torch.Size([18, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([24, 512, 1, 1]).\n\tWhile copying the parameter named \"rpn_cls_score_net.bias\", whose dimensions in the model are torch.Size([18]) and whose dimensions in the checkpoint are torch.Size([24]).\n\tWhile copying the parameter named \"rpn_bbox_pred_net.weight\", whose dimensions in the model are torch.Size([36, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([48, 512, 1, 1]).\n\tWhile copying the parameter named \"rpn_bbox_pred_net.bias\", whose dimensions in the model are torch.Size([36]) and whose dimensions in the checkpoint are torch.Size([48]).\n\tWhile copying the parameter named \"cls_score_net.weight\", whose dimensions in the model are torch.Size([21, 4096]) and whose dimensions in the checkpoint are torch.Size([81, 4096]).\n\tWhile copying the parameter named \"cls_score_net.bias\", whose dimensions in the model are torch.Size([21]) and whose dimensions in the checkpoint are torch.Size([81]).\n\tWhile copying the parameter named \"bbox_pred_net.weight\", whose dimensions in the model are torch.Size([84, 4096]) and whose dimensions in the checkpoint are torch.Size([324, 4096]).\n\tWhile copying the parameter named \"bbox_pred_net.bias\", whose dimensions in the model are torch.Size([84]) and whose dimensions in the checkpoint are torch.Size([324])."
     ]
    }
   ],
   "source": [
    "net = vgg16()\n",
    "#net = resnetv1(num_layers=152)\n",
    "net.create_architecture(21, tag='default', anchor_scales=[8, 16, 32])\n",
    "net.load_state_dict(torch.load(\"vgg16_faster_rcnn_iter_1190000.pth\", map_location=lambda storage, loc: storage))\n",
    "net.eval()\n",
    "if not torch.cuda.is_available():\n",
    "    net._device = 'cpu'\n",
    "net.to(net._device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../controllable-captioning/raw/splits/train_split.json\", \"r\") as f:\n",
    "    train_split = json.load(f)\n",
    "with open(\"../../controllable-captioning/raw/splits/dev_split.json\", \"r\") as f:\n",
    "    dev_split = json.load(f)\n",
    "with open(\"../../controllable-captioning/raw/splits/test_split.json\", \"r\") as f:\n",
    "    test_split = json.load(f)\n",
    "    \n",
    "with open(\"../../controllable-captioning/raw/paragraphs_topics_v1.pickle\", \"rb\") as f:\n",
    "    paragraph_topics = pickle.load(f)\n",
    "    \n",
    "with open(\"../../controllable-captioning/raw/paragraphs_v1.json\", \"r\") as f:\n",
    "    paragraphs_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(paragraphs_json, progress_bar=True):\n",
    "    train_data = []\n",
    "    dev_data = []\n",
    "    test_data = []\n",
    "    image_ids = {}\n",
    "    topic_set = set()\n",
    "    for i, json in enumerate(tqdm_notebook(paragraphs_json) if progress_bar else paragraphs_json):\n",
    "        topic_to_seq = {}\n",
    "        for j, sentence in enumerate(sent_detector.tokenize(json['paragraph'])):\n",
    "            sentence = sentence.strip().lower()\n",
    "            t = nlp(sentence)\n",
    "            image_id = json['image_id']\n",
    "            image_ids[image_id] = i\n",
    "            \n",
    "            if 'perfect_match' in paragraph_topics[i][j]:\n",
    "                topic_list = set([topic[0] for topic in paragraph_topics[i][j]['perfect_match']])\n",
    "                for topic in topic_list:\n",
    "                    topic_set.add(topic)\n",
    "                    if topic not in topic_to_seq:\n",
    "                        topic_to_seq[topic] = []\n",
    "                    topic_to_seq[topic].extend(t)\n",
    "            for topic in topic_to_seq:\n",
    "                if image_id in train_split:\n",
    "                    train_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "                elif image_id in dev_split:\n",
    "                    dev_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "                elif image_id in test_split:\n",
    "                    test_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "    return train_data, dev_data, test_data, image_ids, topic_set\n",
    "                        \n",
    "train_data, dev_data, test_data, image_ids, topic_set = parse_data(paragraphs_json)\n",
    "print(\"Length of train split: %d\" %len(train_data))\n",
    "print(\"Length of dev split: %d\" %len(dev_data))\n",
    "print(\"Length of test split: %d\" %len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"A vocabulary wrapper, contains a word_to_index dictionary and a index_to_word list\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = []\n",
    "        self.index = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word_to_index:\n",
    "            self.word_to_index[word] = self.index\n",
    "            self.index_to_word.append(word)\n",
    "            self.index += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if type(word) == str:\n",
    "            if not word in self.word_to_index:\n",
    "                return self.word_to_index['<UNK>']\n",
    "            return self.word_to_index[word]\n",
    "        else:\n",
    "            return self.index_to_word[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.index\n",
    "\n",
    "def build_vocab(sentences, min_occurrences):\n",
    "    \"\"\"Builds a Vocabulary object\"\"\"\n",
    "    counter = Counter()\n",
    "    for sentence in tqdm_notebook(sentences):\n",
    "        for word in sentence:\n",
    "            counter[word.text] += 1\n",
    "\n",
    "    # a word must appear at least min_occurrence times to be included in the vocabulary\n",
    "    words = [word for word, count in counter.items() if count >= min_occurrences]\n",
    "\n",
    "    # Creating a vocabulary object\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<SOS>')\n",
    "    vocab.add_word('<EOS>')\n",
    "    vocab.add_word('<UNK>')\n",
    "\n",
    "    # Adds the words from the captions to the vocabulary\n",
    "    for word in words:\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "word_vocab = build_vocab([val[2] for val in train_data], 5)\n",
    "topic_vocab = Vocabulary()\n",
    "for topic in topic_set:\n",
    "    topic_vocab.add_word(topic)\n",
    "print(\"Length of word vocab: %d\" %len(word_vocab))\n",
    "print(\"Number of topics: %d\" %len(topic_vocab))\n",
    "print(topic_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchedData(object):\n",
    "    def __init__(self, batch_size):\n",
    "        self.batched_data = []\n",
    "        self.index = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add_batch(self, batch):\n",
    "        if len(batch) == self.batch_size:\n",
    "            self.batched_data.append(batch)\n",
    "        else:\n",
    "            print(\"not the correct size batch!\")\n",
    "\n",
    "    def __call__(self, index):\n",
    "        if not index < len(self.batched_data):\n",
    "            return []\n",
    "        return self.batched_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batched_data)\n",
    "    \n",
    "def batch_data(data, batch_size, max_size=None, progress_bar=True, randomize=True):\n",
    "    batched_set = {}\n",
    "    counter = 0\n",
    "    for (image_id, topic, sentence) in (tqdm_notebook(data) if progress_bar else data):\n",
    "        # accounting for SOS and EOS tokens\n",
    "        sentence = [token.text for token in sentence]\n",
    "        caption_len = len(sentence) + 2\n",
    "        if caption_len not in batched_set.keys():\n",
    "            batched_set[caption_len] = []\n",
    "        batched_set[caption_len].append((image_id, topic, [token for token in sentence]))\n",
    "\n",
    "    batched_data = BatchedData(batch_size)\n",
    "    curr_size = 0\n",
    "\n",
    "    for i in batched_set.keys():\n",
    "        if len(batched_set[i]) >= batch_size:\n",
    "            batch = batched_set[i]\n",
    "            random.shuffle(batch)\n",
    "        for j in range(len(batch) // batch_size):\n",
    "            if max_size is None or curr_size < max_size:\n",
    "                batched_data.add_batch(batch[batch_size * j : batch_size * (j+1)])\n",
    "                curr_size += 1\n",
    "    if randomize:\n",
    "        random.shuffle(batched_data.batched_data)\n",
    "    return batched_data\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "batched_train_data = batch_data(train_data, batch_size, progress_bar=False)\n",
    "batched_dev_data = batch_data(dev_data, batch_size, progress_bar=False)\n",
    "batched_single_dev_data = batch_data(dev_data, 1, progress_bar=False, randomize=False)\n",
    "batched_test_data = batch_data(test_data, 1, progress_bar=False, randomize=False)\n",
    "\n",
    "print(\"number of train batches: %d\" %len(batched_train_data))\n",
    "print(\"number of dev batches: %d\" %len(batched_dev_data))\n",
    "print(\"number of test batches: %d\" %len(batched_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(data.Dataset):\n",
    "    def __init__(self, paragraphs, batched_captions, word_vocab, topic_vocab, transform=None):\n",
    "        \"\"\"\n",
    "        Set the path for images, captions and vocabulary wrapper.\n",
    "    \n",
    "        Args:\n",
    "        \n",
    "                vocab: vocabulary wrapper.\n",
    "                transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.paragraphs = paragraphs\n",
    "        self.batched_captions = batched_captions\n",
    "        self.word_vocab = word_vocab\n",
    "        self.topic_vocab = topic_vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        images = []\n",
    "        topics = []\n",
    "        captions = []\n",
    "        img_ids = []\n",
    "        for (image_id, topic, sentence) in self.batched_captions(index):\n",
    "            #image = Image.open(\"../data/\" + str(image_id) + \".jpg\").convert('RGB')\n",
    "            image = cv2.imread(\"../data/\" + str(image_id) + \".jpg\")\n",
    "\n",
    "            #if self.transform is not None:\n",
    "            #    image = self.transform(image)\n",
    "            images.append(image)\n",
    "            topics.append(self.topic_vocab(topic))\n",
    "            img_ids.append(image_id)\n",
    "            captions.append([self.word_vocab('<SOS>')] + [self.word_vocab(token) for token in sentence] + [self.word_vocab('<EOS>')])\n",
    "\n",
    "        lengths = [len(caption) for caption in captions]\n",
    "        #return torch.stack(images, 0), torch.LongTensor(topics), torch.LongTensor(captions), lengths, img_ids\n",
    "        return images, torch.LongTensor(topics), torch.LongTensor(captions), lengths, img_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batched_captions)\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "  \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging captions (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    images, topics, captions, lengths, image_ids = zip(*data)\n",
    "    return images[0], topics[0], captions[0], lengths[0], image_ids[0]\n",
    "\n",
    "\n",
    "def get_loader(paragraphs, batched_data, word_vocab, topic_vocab, transform, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    data_set = CustomDataSet(paragraphs, batched_data, word_vocab, topic_vocab, transform)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=data_set, \n",
    "                                            shuffle=shuffle,\n",
    "                                            num_workers=num_workers,\n",
    "                                            collate_fn=collate_fn)\n",
    "    return data_loader\n",
    "\n",
    "single_val_data_loader = get_loader(dev_data, batched_single_dev_data, word_vocab, topic_vocab, transform, False, 2)\n",
    "test_data_loader = get_loader(test_data, batched_test_data, word_vocab, topic_vocab, transform, False, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x, useCuda=True, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, topics, captions, lengths, ids in single_val_data_loader:\n",
    "    #images = to_var(images, volatile=False)\n",
    "    topics = to_var(topics, volatile=False)\n",
    "    scores, boxes = im_detect(net, images[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
