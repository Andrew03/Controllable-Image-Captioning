{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Packages and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:27:14.837338Z",
     "start_time": "2018-05-27T08:27:13.324753Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from IPython.display import Image as DisplayImage\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import spacy\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# GPU Selection\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:27:22.105153Z",
     "start_time": "2018-05-27T08:27:18.879846Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/raw/splits/train_split.json\", \"r\") as f:\n",
    "    train_split = json.load(f)\n",
    "with open(\"data/raw/splits/dev_split.json\", \"r\") as f:\n",
    "    dev_split = json.load(f)\n",
    "with open(\"data/raw/splits/test_split.json\", \"r\") as f:\n",
    "    test_split = json.load(f)\n",
    "\n",
    "with open(\"data/raw/paragraphs_topics_v1.pickle\", \"rb\") as f:\n",
    "    paragraph_topics = pickle.load(f)\n",
    "with open(\"data/raw/paragraphs_v1.json\", \"r\") as f:\n",
    "    paragraphs_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:38:09.576304Z",
     "start_time": "2018-05-27T08:27:37.327951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1daae57161d445fa7c055049a9d190b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19561), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of train split: 269156\n",
      "Length of dev split: 47031\n",
      "Length of test split: 46013\n"
     ]
    }
   ],
   "source": [
    "def parse_data(paragraphs_json, progress_bar=True):\n",
    "    train_data = []\n",
    "    dev_data = []\n",
    "    test_data = []\n",
    "    image_ids = {}\n",
    "    topic_set = set()\n",
    "    for i, json in enumerate(tqdm_notebook(paragraphs_json) if progress_bar else paragraphs_json):\n",
    "        topic_to_seq = {}\n",
    "        for j, sentence in enumerate(sent_detector.tokenize(json['paragraph'])):\n",
    "            sentence = sentence.strip().lower()\n",
    "            t = nlp(sentence)\n",
    "            image_id = json['image_id']\n",
    "            image_ids[image_id] = i\n",
    "\n",
    "            if 'perfect_match' in paragraph_topics[i][j]:\n",
    "                topic_list = set([\n",
    "                    topic[0]\n",
    "                    for topic in paragraph_topics[i][j]['perfect_match']\n",
    "                ])\n",
    "                for topic in topic_list:\n",
    "                    topic_set.add(topic)\n",
    "                    if topic not in topic_to_seq:\n",
    "                        topic_to_seq[topic] = []\n",
    "                    topic_to_seq[topic].extend(t)\n",
    "            for topic in topic_to_seq:\n",
    "                if image_id in train_split:\n",
    "                    train_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "                elif image_id in dev_split:\n",
    "                    dev_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "                elif image_id in test_split:\n",
    "                    test_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "    return train_data, dev_data, test_data, image_ids, topic_set\n",
    "\n",
    "train_data, dev_data, test_data, image_ids, topic_set = parse_data(paragraphs_json)\n",
    "print(\"Length of train split: %d\" % len(train_data))\n",
    "print(\"Length of dev split: %d\" % len(dev_data))\n",
    "print(\"Length of test split: %d\" % len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:38:59.317630Z",
     "start_time": "2018-05-27T08:38:59.312787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'url': u'https://cs.stanford.edu/people/rak248/VG_100K/2317429.jpg', u'image_id': 2317429, u'paragraph': u'A white round plate is on a table with a plastic tablecloth on it.  Two foil covered food halves are on the white plate along with a serving of golden yellow french fries.  Next to the white plate in a short,  topless, plastic container is a white sauce.  Diagonal to the white plate are the edges of several other stacked plates.  There are black shadows reflected on the table.'}\n",
      "1\n",
      "(2317429, 'color', [a, white, round, plate, is, on, a, table, with, a, plastic, tablecloth, on, it, ., two, foil, covered, food, halves, are, on, the, white, plate, along, with, a, serving, of, golden, yellow, french, fries, ., next, to, the, white, plate, in, a, short, ,,  , topless, ,, plastic, container, is, a, white, sauce, ., diagonal, to, the, white, plate, are, the, edges, of, several, other, stacked, plates, ., there, are, black, shadows, reflected, on, the, table, .])\n",
      "set(['body', 'emotion', 'transportation', 'people', 'color', 'signage', 'food', 'setting', 'animal', 'activity', 'weather', 'clothing'])\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs_json[1])\n",
    "print(image_ids[(train_data[3][0])])\n",
    "print(train_data[3])\n",
    "print(topic_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:39:59.249617Z",
     "start_time": "2018-05-27T08:39:53.037607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f170f4945947e4bfcbfbd2a16b2e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=269156), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of word vocab: 10035\n",
      "Number of topics: 12\n",
      "set(['body', 'emotion', 'transportation', 'people', 'color', 'signage', 'food', 'setting', 'animal', 'activity', 'weather', 'clothing'])\n"
     ]
    }
   ],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"A vocabulary wrapper, contains a word_to_index dictionary and a index_to_word list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = []\n",
    "        self.index = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word_to_index:\n",
    "            self.word_to_index[word] = self.index\n",
    "            self.index_to_word.append(word)\n",
    "            self.index += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if type(word) == int:\n",
    "            return self.index_to_word[word]\n",
    "        else:\n",
    "            if not word in self.word_to_index:\n",
    "                return self.word_to_index['<UNK>']\n",
    "            return self.word_to_index[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.index\n",
    "\n",
    "\n",
    "def build_vocab(sentences, min_occurrences):\n",
    "    \"\"\"Builds a Vocabulary object\"\"\"\n",
    "    counter = Counter()\n",
    "    for sentence in tqdm_notebook(sentences):\n",
    "        for word in sentence:\n",
    "            counter[word.text] += 1\n",
    "\n",
    "    # a word must appear at least min_occurrence times to be included in the vocabulary\n",
    "    words = [\n",
    "        word for word, count in counter.items() if count >= min_occurrences\n",
    "    ]\n",
    "\n",
    "    # Creating a vocabulary object\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<SOS>')\n",
    "    vocab.add_word('<EOS>')\n",
    "    vocab.add_word('<UNK>')\n",
    "\n",
    "    # Adds the words from the captions to the vocabulary\n",
    "    for word in words:\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "word_vocab = build_vocab([val[2] for val in train_data], 5)\n",
    "topic_vocab = Vocabulary()\n",
    "for topic in topic_set:\n",
    "    topic_vocab.add_word(topic)\n",
    "print(\"Length of word vocab: %d\" % len(word_vocab))\n",
    "print(\"Number of topics: %d\" % len(topic_vocab))\n",
    "print(topic_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the Data\n",
    "The data is batched so sentences of the same length are grouped together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:40:32.066328Z",
     "start_time": "2018-05-27T08:40:24.808963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train batches: 16743\n",
      "number of dev batches: 2869\n",
      "number of test batches: 46013\n"
     ]
    }
   ],
   "source": [
    "class BatchedData(object):\n",
    "    def __init__(self, batch_size):\n",
    "        self.batched_data = []\n",
    "        self.index = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add_batch(self, batch):\n",
    "        if len(batch) == self.batch_size:\n",
    "            self.batched_data.append(batch)\n",
    "        else:\n",
    "            print(\"not the correct size batch!\")\n",
    "\n",
    "    def __call__(self, index):\n",
    "        if not index < len(self.batched_data):\n",
    "            return []\n",
    "        return self.batched_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batched_data)\n",
    "\n",
    "def batch_data(data, batch_size, progress_bar=True, randomize=True, max_size=None):\n",
    "    batched_set = {}\n",
    "    counter = 0\n",
    "    for (image_id, topic, sentence) in (tqdm_notebook(data) if progress_bar else data):\n",
    "        # accounting for SOS and EOS tokens\n",
    "        sentence = [token.text for token in sentence]\n",
    "        caption_len = len(sentence) + 2\n",
    "        if caption_len not in batched_set.keys():\n",
    "            batched_set[caption_len] = []\n",
    "        batched_set[caption_len].append((image_id, topic, [token for token in sentence]))\n",
    "\n",
    "    batched_data = BatchedData(batch_size)\n",
    "\n",
    "    curr_size = 0\n",
    "    for i in batched_set.keys():\n",
    "        if len(batched_set[i]) >= batch_size:\n",
    "            batch = batched_set[i]\n",
    "            if randomize:\n",
    "                random.shuffle(batch)\n",
    "            for j in range(len(batch) // batch_size):\n",
    "                if max_size is not None and curr_size == max_size:\n",
    "                    return batched_data\n",
    "                batched_data.add_batch(batch[batch_size * j:batch_size * (j + 1)])\n",
    "                curr_size += 1\n",
    "    if randomize:\n",
    "        random.shuffle(batched_data.batched_data)\n",
    "    return batched_data\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "batched_train_data = batch_data(train_data, batch_size, progress_bar=False)\n",
    "batched_dev_data = batch_data(dev_data, batch_size, progress_bar=False)\n",
    "batched_single_dev_data = batch_data(dev_data, 1, progress_bar=False, randomize=False)\n",
    "batched_test_data = batch_data(test_data, 1, progress_bar=False, randomize=False)\n",
    "\n",
    "print(\"number of train batches: %d\" % len(batched_train_data))\n",
    "print(\"number of dev batches: %d\" % len(batched_dev_data))\n",
    "print(\"number of test batches: %d\" % len(batched_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transform\n",
    "The transformation that will be applied to all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:41:00.766043Z",
     "start_time": "2018-05-27T08:41:00.757624Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:41:27.719964Z",
     "start_time": "2018-05-27T08:41:27.686009Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataSet(data.Dataset):\n",
    "    def __init__(self, paragraphs,  batched_captions, word_vocab, topic_vocab, transform=None):\n",
    "        \"\"\"\n",
    "        Set the path for images, captions and vocabulary wrapper.\n",
    "    \n",
    "        Args:\n",
    "        \n",
    "                vocab: vocabulary wrapper.\n",
    "                transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.paragraphs = paragraphs\n",
    "        self.batched_captions = batched_captions\n",
    "        self.word_vocab = word_vocab\n",
    "        self.topic_vocab = topic_vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        images = []\n",
    "        topics = []\n",
    "        captions = []\n",
    "        img_ids = []\n",
    "        for (image_id, topic, sentence) in self.batched_captions(index):\n",
    "            image = Image.open(\"data/images/%d.jpg\" % image_id).convert('RGB')\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "            images.append(image)\n",
    "            topics.append(self.topic_vocab(topic))\n",
    "            img_ids.append(image_id)\n",
    "            captions.append([self.word_vocab('<SOS>')] +\n",
    "                            [self.word_vocab(token) for token in sentence] +\n",
    "                            [self.word_vocab('<EOS>')])\n",
    "\n",
    "        lengths = [len(caption) for caption in captions]\n",
    "        return torch.stack(images, 0), torch.LongTensor(\n",
    "            topics), torch.LongTensor(captions), lengths, img_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batched_captions)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "  \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging captions (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    images, topics, captions, lengths, image_ids = zip(*data)\n",
    "    return images[0], topics[0], captions[0], lengths[0], image_ids[0]\n",
    "\n",
    "\n",
    "def get_loader(paragraphs, batched_data, word_vocab, topic_vocab, transform,\n",
    "               shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    data_set = CustomDataSet(paragraphs, batched_data, word_vocab, topic_vocab,\n",
    "                             transform)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=data_set,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn)\n",
    "    return data_loader\n",
    "\n",
    "test_data_loader = get_loader(test_data, batched_test_data, word_vocab,\n",
    "                              topic_vocab, transform, False, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model\n",
    "## The Encoder\n",
    "The encoder is a CNN, specifically VGG-16 with which we strip off the final few layers to get 196 visual feature vectors of size 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:41:55.343361Z",
     "start_time": "2018-05-27T08:41:54.186048Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        vgg = models.vgg16(pretrained=True).eval()\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = nn.Sequential(*(vgg.features[i] for i in range(29)))\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.vgg(images)\n",
    "        features_reshaped = features.view(-1, 512, 196)\n",
    "        features_transposed = features_reshaped.transpose(1, 2)\n",
    "        return features_transposed\n",
    "\n",
    "\n",
    "encoder_cnn = EncoderCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder\n",
    "The decoder is an LSTM with attention and dropout. We compute the initial hidden state and cell state as follows:\n",
    "$$h_{0} = \\tanh((\\frac{1}{196} \\sum_{i=0}^{196} f_{initH}(a_{i})) + g_{initH}(t_{j}))$$\n",
    "$$c_{0} = \\tanh((\\frac{1}{196} \\sum_{i=0}^{196} f_{initC}(a_{i})) + g_{initC}(t_{j}))$$\n",
    "where $f_{initH}, f_{initC}, g_{initH}, g_{initC}$ are all linear functions, $a_{i}$ is the $i^{th}$ annotation vector and $t_{j}$ is the topic embedding.\n",
    "We compute attention using the soft attention model from Show Tell Attend as follows:\n",
    "$$e_{ti} = w_{1} a_{i} + w_{2} h_{t-1} + w_{3} t_{j}$$\n",
    "$$\\alpha_{ti} = \\frac{\\exp(e_{ti})}{\\sum_{k=1}^{L} \\exp(e_{tk})}$$\n",
    "and the context vector $\\hat{z_{t}} = \\sum_{i=1}^{L} \\alpha_{ti} a_{i}$,\n",
    "where $w_{1}, w_{2}, w_{3}$ are all learned matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:42:22.293134Z",
     "start_time": "2018-05-27T08:42:21.984655Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_predict_input_captions(captions):\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.LongTensor(captions)\n",
    "    return torch.LongTensor(captions)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vis_dim: The size of each visual feature vector\n",
    "        vis_num: The number of visual feature vectors\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vis_dim, vis_num, embed_dim, hidden_dim, \n",
    "                 word_vocab_size, topic_vocab_size, num_layers=1, dropout=0.0):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.vis_dim = vis_dim\n",
    "        self.vis_num = vis_num\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        self.topic_vocab_size = topic_vocab_size\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.init_h_vis = nn.Linear(vis_dim, hidden_dim, bias=False)\n",
    "        self.init_c_vis = nn.Linear(vis_dim, hidden_dim, bias=False)\n",
    "        self.init_h_topic = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.init_c_topics = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.attn_vw = nn.Linear(vis_dim, 1)\n",
    "        self.attn_hw = nn.Linear(hidden_dim, 1)\n",
    "        self.attn_tw = nn.Linear(embed_dim, 1)\n",
    "\n",
    "        self.topic_embed = nn.Embedding(topic_vocab_size, embed_dim)\n",
    "        self.word_embed = nn.Embedding(word_vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(vis_dim + embed_dim, hidden_dim, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_dim, word_vocab_size)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "\n",
    "    def _init_hidden(self, features, topics):\n",
    "        tanh = nn.Tanh()\n",
    "        hidden = ((tanh(torch.sum(self.init_h_vis(features), 1) / self.vis_num)) + self.init_h_topic(topics))\n",
    "        cell = ((tanh(torch.sum(self.init_c_vis(features), 1) / self.vis_num)) + self.init_c_topics(topics))\n",
    "        return hidden.unsqueeze(0), cell.unsqueeze(0)\n",
    "\n",
    "    def _compute_attention(self, features, topics, hidden_state):\n",
    "        \"\"\"\n",
    "        features: B x vis_num x vis_dim\n",
    "        hidden_state: (1 x B x hidden_size, 1 x B x hidden_size)\n",
    "        \"\"\"\n",
    "        # add in L1 norm (sum up everything and divide everything by sum\n",
    "        #features = torch.norm(features, 1, 2, )\n",
    "        # B x vis_num x 1\n",
    "        att_vw = self.attn_vw(features)\n",
    "        # B x vis_num x 1\n",
    "        att_hw = self.attn_hw(\n",
    "            hidden_state.transpose(0, 1).repeat(1, self.vis_num, 1))\n",
    "        att_tw = self.attn_tw(topics).unsqueeze(1).repeat(1, self.vis_num, 1)\n",
    "        # B x vis_num x 1\n",
    "        attention = att_vw + att_hw + att_tw\n",
    "        attention_softmax = F.softmax(attention, dim=1)\n",
    "        # B x vis_dim\n",
    "        # also return the attention_softmax and average those\n",
    "        return torch.sum(features * attention_softmax, 1), attention_softmax\n",
    "\n",
    "    def forward(self, features, topics, captions):\n",
    "        \"\"\"\n",
    "        topic: B x 1\n",
    "        features: B x vis_num x vis_dim\n",
    "        captions: B x seq_length\n",
    "        \"\"\"\n",
    "        topic_embeddings = self.topic_embed(topics)\n",
    "        hidden = self._init_hidden(features, topic_embeddings)\n",
    "        word_embeddings = self.word_embed(captions)\n",
    "        word_space = None\n",
    "        lengths = len(captions[0])\n",
    "        average_attention = None\n",
    "        for i in range(lengths):\n",
    "            embedding = torch.index_select(\n",
    "                word_embeddings, 1, torch.cuda.LongTensor([i]))\n",
    "            attention, alpha = self._compute_attention(features, topic_embeddings, hidden[0])\n",
    "            attention = attention.unsqueeze(1)\n",
    "            #average_attention = alpha if average_attention is None else average_attention + alpha\n",
    "            input = self.dropout_layer(torch.cat([attention, embedding], 2))\n",
    "            out, hidden = self.lstm(input, hidden)\n",
    "            words = self.output(self.dropout_layer(out))\n",
    "            word_space = torch.cat([word_space, words], 1) if word_space is not None else words\n",
    "        word_space = pack_padded_sequence(\n",
    "            word_space, [lengths for i in range(len(captions))],\n",
    "            batch_first=True)[0]\n",
    "        return F.log_softmax(\n",
    "            word_space, dim=1), F.softmax(\n",
    "                word_space, dim=1), #average_attention\n",
    "\n",
    "    def sample(self, features, topics, beam_size=1, start_token=0, end_token=1):\n",
    "        topic_embeddings = self.topic_embed(topics)\n",
    "        hidden = self._init_hidden(features, topic_embeddings)\n",
    "        completed_phrases = []\n",
    "        best_phrases = []\n",
    "        score = 0\n",
    "\n",
    "        initial_caption = create_predict_input_captions([start_token])\n",
    "        embedding = self.word_embed(initial_caption)\n",
    "        attention, _ = self._compute_attention(features, topic_embeddings,\n",
    "                                               hidden[0])\n",
    "        input = torch.cat([attention, embedding], 1).unsqueeze(1)\n",
    "        out, hidden = self.lstm(input, hidden)\n",
    "        words = self.output(out)\n",
    "        word_scores = F.softmax(words, dim=2)\n",
    "        top_scores, top_captions = word_scores.topk(beam_size)\n",
    "        best_phrases = [[\n",
    "            top_scores[0][0].data[i], [top_captions[0][0].data[i]]\n",
    "        ] for i in range(beam_size)]\n",
    "        next_captions = top_captions.resize(beam_size, 1)\n",
    "        hidden = (hidden[0].repeat(1, beam_size, 1), hidden[1].repeat(\n",
    "            1, beam_size, 1))\n",
    "\n",
    "        for index in range(40):\n",
    "            best_candidates = []\n",
    "            embedding = self.word_embed(next_captions)\n",
    "            attention, _ = self._compute_attention(features, topic_embeddings,\n",
    "                                                   hidden[0])\n",
    "            attention = attention.unsqueeze(1)\n",
    "            input = torch.cat([attention, embedding], 2)\n",
    "            out, hidden = self.lstm(input, hidden)\n",
    "            words = self.output(out)\n",
    "            word_scores = F.softmax(words, dim=2)\n",
    "            top_scores, top_captions = word_scores.topk(beam_size)\n",
    "            len_phrases = len(best_phrases[0][1])\n",
    "            for i in range(len(best_phrases)):\n",
    "                for j in range(beam_size):\n",
    "                    best_candidates.extend([[\n",
    "                        best_phrases[i][0] + top_scores[i][0].data[j],\n",
    "                        best_phrases[i][1] + [top_captions[i][0].data[j]], i\n",
    "                    ]])\n",
    "            top_candidates = sorted(\n",
    "                best_candidates,\n",
    "                key=lambda score_caption: score_caption[0])[-beam_size:]\n",
    "            temp_candidates = []\n",
    "            for phrase in top_candidates:\n",
    "                if phrase[1][-1] == end_token:\n",
    "                    completed_phrases.append(\n",
    "                        [phrase[0] / len(phrase[1]), phrase[1]])\n",
    "                else:\n",
    "                    temp_candidates.append(phrase)\n",
    "            top_candidates = temp_candidates\n",
    "            if len(completed_phrases) >= beam_size:\n",
    "                return sorted(\n",
    "                    completed_phrases,\n",
    "                    key=lambda score_caption: score_caption[0],\n",
    "                    reverse=True)[:beam_size]\n",
    "            best_phrases = [[phrase[0], phrase[1]]\n",
    "                            for phrase in top_candidates]\n",
    "            next_captions = create_predict_input_captions(\n",
    "                [[phrase[1][-1]] for phrase in top_candidates])\n",
    "            hidden_0 = (torch.stack([\n",
    "                hidden[0][0].select(0, phrase[2]) for phrase in top_candidates\n",
    "            ]).unsqueeze(0))\n",
    "            hidden_1 = (torch.stack([\n",
    "                hidden[1][0].select(0, phrase[2]) for phrase in top_candidates\n",
    "            ]).unsqueeze(0))\n",
    "            hidden = (hidden_0, hidden_1)\n",
    "        return sorted(\n",
    "            completed_phrases,\n",
    "            key=lambda score_caption: score_caption[0],\n",
    "            reverse=True)[:beam_size] if len(completed_phrases) != 0 else top_candidates\n",
    "\n",
    "\n",
    "decoder_rnn = DecoderRNN(512, 196, 512, 512, len(word_vocab), len(topic_vocab), num_layers=2, dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:42:53.962801Z",
     "start_time": "2018-05-27T08:42:49.071955Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    encoder_cnn.cuda()\n",
    "    decoder_rnn.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Loss Function and  Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:43:20.657449Z",
     "start_time": "2018-05-27T08:43:20.650488Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "params = list(decoder_rnn.parameters())\n",
    "optimizer = optim.Adam(params, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Saved Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:35.881641Z",
     "start_time": "2018-05-26T10:51:35.875463Z"
    }
   },
   "outputs": [],
   "source": [
    "load_checkpoint = None\n",
    "if load_checkpoint is not None:\n",
    "    checkpoint = torch.load(load_checkpoint)\n",
    "    print(\"loading from checkpoint \" + str(load_checkpoint))\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    decoder_rnn.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    del checkpoint\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:35.912659Z",
     "start_time": "2018-05-26T10:51:35.885187Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(images, topics, captions, encoder_cnn, decoder_rnn, loss_function):\n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()     \n",
    "        topics = topics.cuda()\n",
    "        captions = captions.cuda()\n",
    "        \n",
    "    inputs = captions[:, :-1]\n",
    "    targets = captions[:, 1:]   \n",
    "    features = encoder_cnn(images)\n",
    "    len_targets = len(targets[0])\n",
    "    targets = pack_padded_sequence(\n",
    "        targets, [len_targets for i in range(len(captions))],\n",
    "        batch_first=True)[0]\n",
    "    predictions, _ = decoder_rnn(features, topics, inputs)\n",
    "    loss = loss_function(predictions, targets)\n",
    "    return loss#, average_attention\n",
    "\n",
    "\n",
    "def train(images, topics, captions, encoder_cnn, decoder_rnn, loss_function,\n",
    "          optimizer, grad_clip):\n",
    "    decoder_rnn.train()\n",
    "    decoder_rnn.zero_grad()\n",
    "    # added in requires grad\n",
    "    loss = evaluate(images, topics.requires_grad_(), captions.requires_grad_(), encoder_cnn, decoder_rnn, loss_function)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(decoder_rnn.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "def val(images, topics, captions, encoder_cnn, decoder_rnn, loss_function):\n",
    "    decoder_rnn.eval()\n",
    "    loss = evaluate(images, topics, captions, encoder_cnn, decoder_rnn, loss_function)\n",
    "    return loss.data.item()#, average_attention.squeeze(0).squeeze(1).data\n",
    "\n",
    "\n",
    "def caption_id_to_string(caption, vocab):\n",
    "    output = \"\"\n",
    "    for word in caption:\n",
    "        if \".\" != word and \"<EOS>\" != word:\n",
    "            output += vocab(word) + \" \"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:35.925667Z",
     "start_time": "2018-05-26T10:51:35.916184Z"
    }
   },
   "outputs": [],
   "source": [
    "basedir = \"Attention With Topic, Tanh After Both Components\"\n",
    "output_train_data = {}\n",
    "output_val_data = {}\n",
    "bleu_score_data = {}\n",
    "rouge_score_data = {}\n",
    "cider_score_data = {}\n",
    "attention_val_data = {}\n",
    "\n",
    "if load_checkpoint is not None:\n",
    "    with open(\"data/outputs/{}/train_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        output_train_data = pickle.load(f)\n",
    "    with open(\"data/outputs/{}/val_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        output_val_data = pickle.load(f)\n",
    "    with open(\"data/outputs/{}/bleu_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        bleu_score_data = pickle.load(f)\n",
    "    with open(\"data/outputs/{}/rouge_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        rouge_score_data = pickle.load(f)\n",
    "    with open(\"data/outputs/{}/cider_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        cider_score_data = pickle.load(f)\n",
    "    with open(\"data/outputs/{}/attention_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        attention_val_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.421693Z",
     "start_time": "2018-05-26T10:51:35.937369Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d00067b6354bd8b1b6d3ed14fb764f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Train [11/20]', max=16743), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f9129439a50>> ignored\n",
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f91e16d2e90>> ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150c60e1e74648898f881c25179b1b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Val [11/20]', max=2869), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e667566f6e1a42f7ba2999dfaf27dd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Train [12/20]', max=16743), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f91310a74d0>> ignored\n",
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f91dd74cb10>> ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09f222fa2514b46b13373b38e78175f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Val [12/20]', max=2869), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8015576f7c4c95be5e12d66d617866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Train [13/20]', max=16743), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f90e2bd26d0>> ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db83987c6834ebf8781740ad25d383d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Val [13/20]', max=2869), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e87c31e17a4ab493b4510caf1773d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Train [14/20]', max=16743), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f90ec3e9950>> ignored\n",
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f91d1743fd0>> ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbf06c491704c1d966e444efb5e4ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Val [14/20]', max=2869), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f70be18341e4b49822b98ace7fff537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Train [15/20]', max=16743), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f91310a7210>> ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e537c567b9e84d55b38974ae75904a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Val [16/20]', max=2869), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcba9584d9d439c83fe82f354644733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Train [17/20]', max=16743), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f90e68e8310>> ignored\n",
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f90e68e8310>> ignored\n",
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f90f75f4210>> ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48753ebef82c456ca69ca03b5617e37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Val [17/20]', max=2869), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e801f907e25642d0b58ba3ca0143d2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Train [20/20]', max=16743), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f90bbe75510>> ignored\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b16cd0b25b44527a0e55dbf8bef0906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Val [20/20]', max=2869), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 10\n",
    "num_epochs = 20\n",
    "grad_clip = 5.0\n",
    "beam_size=10\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # Reshuffle data\n",
    "    batched_train_data = batch_data(train_data, batch_size, progress_bar=False)\n",
    "    batched_dev_data = batch_data(dev_data, batch_size, progress_bar=False)\n",
    "    \n",
    "    train_data_loader = get_loader(train_data, batched_train_data, word_vocab, topic_vocab, transform, shuffle=True, num_workers=2)\n",
    "    temp_val_loader = get_loader(dev_data, batched_dev_data, word_vocab, topic_vocab, transform, shuffle=True, num_workers=2)\n",
    "\n",
    "    progress_bar = tqdm_notebook(iterable=train_data_loader, desc='Train [%i/%i]' %(epoch + 1 , end_epoch))\n",
    "    train_sum_loss = 0.0\n",
    "    \n",
    "    for i, (images, topics, captions, lengths, ids) in enumerate(progress_bar, 1):\n",
    "        train_sum_loss += train(images, topics, captions, encoder_cnn, decoder_rnn, loss_function, optimizer, grad_clip)\n",
    "        del images, topics, captions, lengths\n",
    "        torch.cuda.empty_cache()\n",
    "        progress_bar.set_postfix(loss=train_sum_loss/(i % 100 if i % 100 != 0 else 1))\n",
    "        if i % 100 == 0:\n",
    "            output_train_data[epoch * len(train_data_loader) + i] = train_sum_loss / 100\n",
    "            train_sum_loss = 0\n",
    "        if i % 2000 == 0:\n",
    "            temp_val_loss = 0.0\n",
    "            for j, (images, topics, captions, lengths, ids) in enumerate(temp_val_loader):\n",
    "                if j == 100:\n",
    "                    break\n",
    "                temp_val_loss += val(images, topics, captions, encoder_cnn, decoder_rnn, loss_function)\n",
    "                del images, topics, captions, lengths\n",
    "                torch.cuda.empty_cache()\n",
    "            output_val_data[epoch * len(train_data_loader) + i] = temp_val_loss / 100\n",
    "    \n",
    "    # end of batch\n",
    "    if len(train_data_loader) % 100 != 0:\n",
    "        output_train_data[(epoch + 1) * len(train_data_loader)] = train_sum_loss / (len(train_data_loader) % 100)\n",
    "    \n",
    "    # validation loss\n",
    "    val_sum_loss = 0\n",
    "    val_data_loader = get_loader(dev_data, batched_dev_data, word_vocab, topic_vocab, transform, shuffle=False, num_workers=2)\n",
    "    val_progress_bar = tqdm_notebook(iterable=val_data_loader, desc='Val [%i/%i]' %(epoch + 1, end_epoch))\n",
    "    for i, (images, topics, captions, lengths, ids) in enumerate(val_progress_bar, 1):\n",
    "        loss = val(images, topics, captions, encoder_cnn, decoder_rnn, loss_function)\n",
    "        # attention_val_data[(epoch + 1, ids[0])] = average_attention\n",
    "        val_sum_loss += loss\n",
    "        val_progress_bar.set_postfix(loss=val_sum_loss/i)\n",
    "        del images, topics, captions, lengths\n",
    "        torch.cuda.empty_cache()\n",
    "    output_val_data[(epoch + 1) * len(train_data_loader)] = val_sum_loss / len(val_data_loader)\n",
    "    \n",
    "    torch.save({'epoch': epoch + 1,\n",
    "            'state_dict': decoder_rnn.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()},\n",
    "            \"data/checkpoints/{}/checkpoint_{}.pt\".format(basedir, epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Evaluation Metrics\n",
    "We use beam search with a beam size of 5 to generate captions and score them based off Bleu, Cider and Rouge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:35.935199Z",
     "start_time": "2018-05-26T10:51:35.928771Z"
    }
   },
   "outputs": [],
   "source": [
    "scorer_bleu = Bleu(4)\n",
    "scorer_rouge = Rouge()\n",
    "scorer_cider = Cider()\n",
    "\n",
    "# Initialize dictionary for reference sequences and generated sequences\n",
    "# This is just the format these need to be in to be passed to the script\n",
    "sequences_ref = {}\n",
    "sequences_gen = {}\n",
    "\n",
    "# Define words that you don't want to count as correct\n",
    "# Things like start tokens, end tokens, <unk> tokens, <pad> tokens, etc.\n",
    "bad_words = ['<SOS>', '<EOS>', '<UNK>']\n",
    "\n",
    "# Convert list of tokens to list of token_indices in vocab\n",
    "bad_toks = [word_vocab(i) for i in bad_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from checkpoint data/checkpoints/Attention With Topic, Tanh After Both Components/checkpoint_5.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1826fa729147abb26003daee2e9b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47031), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5, 20, 5):\n",
    "    load_checkpoint = \"data/checkpoints/{}/checkpoint_{}.pt\".format(basedir, epoch)\n",
    "    checkpoint = torch.load(load_checkpoint)\n",
    "    print(\"loading from checkpoint \" + str(load_checkpoint))\n",
    "    decoder_rnn.load_state_dict(checkpoint['state_dict'])\n",
    "    del checkpoint\n",
    "    torch.cuda.empty_cache()\n",
    "    # generating captions\n",
    "\n",
    "    batched_single_dev_data = batch_data(dev_data, 1, progress_bar=False, randomize=False)\n",
    "    single_val_data_loader = get_loader(dev_data, batched_single_dev_data,\n",
    "                                        word_vocab, topic_vocab, transform, False, 2)\n",
    "    generate_progress_bar = tqdm_notebook(iterable=single_val_data_loader)\n",
    "    for i, (images, topics, captions, lengths, ids) in enumerate(generate_progress_bar, 1):\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            topics = topics.cuda()\n",
    "        features = encoder_cnn(images)\n",
    "        results = decoder_rnn.sample(features, topics, beam_size)\n",
    "        sequences_ref[i] = [\" \".join([word_vocab(j.item()) for j in captions[0]\n",
    "                            if j not in bad_toks])]\n",
    "        sequences_gen[i] = [\" \".join([word_vocab(j.item()) for j in results[0][1]\n",
    "                            if j not in bad_toks])]\n",
    "        del images, topics, captions, lengths\n",
    "        torch.cuda.empty_cache()\n",
    "    bleu_score, bleu_scores = scorer_bleu.compute_score(\n",
    "        sequences_ref, sequences_gen)\n",
    "    bleu_score_data[epoch] = bleu_score\n",
    "    rouge_score, rouge_scores = scorer_rouge.compute_score(\n",
    "        sequences_ref, sequences_gen)\n",
    "    rouge_score_data[epoch] = rouge_score\n",
    "    cider_score, cider_scores = scorer_cider.compute_score(\n",
    "        sequences_ref, sequences_gen)\n",
    "    cider_score_data[epoch] = cider_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu at epoch 5: [0.06698623705170005, 0.034526287913845516, 0.02072496256273058, 0.012717843314362348]\n",
      "Rouge at epoch 5: 0.217459661081\n",
      "Cider at epoch 5: 0.115303611687\n",
      "Bleu at epoch 10: [0.07043316825017887, 0.03641644347596119, 0.022006396643085055, 0.013355134721298456]\n",
      "Rouge at epoch 10: 0.219391092601\n",
      "Cider at epoch 10: 0.100583652435\n",
      "Bleu at epoch 15: [0.06556572887280691, 0.0332330379563886, 0.019844453748935755, 0.012158824381263865]\n",
      "Rouge at epoch 15: 0.219429352442\n",
      "Cider at epoch 15: 0.112635851611\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(bleu_score_data.keys()):\n",
    "    print(\"Bleu at epoch {}: {}\".format(key, bleu_score_data[key]))\n",
    "    print(\"Rouge at epoch {}: {}\".format(key, rouge_score_data[key]))\n",
    "    print(\"Cider at epoch {}: {}\".format(key, cider_score_data[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.423374Z",
     "start_time": "2018-05-26T10:50:43.061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f90f4580e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_trial_number = sorted(output_train_data.keys())\n",
    "train_trial_loss = [output_train_data[i] for i in train_trial_number]\n",
    "val_trial_number = sorted(output_val_data.keys())\n",
    "val_trial_loss = [output_val_data[i] for i in val_trial_number]\n",
    "\n",
    "\n",
    "plt.plot(train_trial_number, train_trial_loss, 'ro', markersize=1, label='Training Batch Loss')\n",
    "plt.plot(val_trial_number, val_trial_loss, 'bo', markersize=5, label='Validation Loss After 1 Full Epoch')\n",
    "\n",
    "plt.xlabel('Number of Batches')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over ' + str(end_epoch) + ' Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.424442Z",
     "start_time": "2018-05-26T10:50:43.294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cs.stanford.edu/people/rak248/VG_100K/2354514.jpg\n",
      "actual caption is: \n",
      "a man is taking a picture of two women.\n",
      "the man is using a canon brand camera based on the camera band.\n",
      "he is also wearing a sleeveless shirt that is blue and white.\n",
      "the two women are tennis players.\n",
      "they are conversing with each other.\n",
      "the one woman is wearing an orange cap and white outfit.\n",
      "the other woman has dark brown hair with a black headband, seemingly wearing an identical outfit to the other woman.\n",
      "the dark haired woman is also holding an orange tennis rackets.\n",
      "behind the women is a green and white sign.\n",
      "the sign seems to be mounted onto a short blue wall.\n",
      "behind the wall are grey metal seats, and next to the seats a column of stairs.\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f91c3d7ca50>> ignored\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<UNK>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-0faebfb4c19d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mrandom_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_topic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing changing topic to %s:\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_topic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e566faee100d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '<UNK>'"
     ]
    }
   ],
   "source": [
    "beam_size = 10\n",
    "batched_temp_data = batch_data(train_data, 1, progress_bar=False)\n",
    "temp_data_loader = get_loader(train_data, batched_temp_data, word_vocab, topic_vocab, transform, True, 2)\n",
    "for images, topics, captions, lengths, ids in temp_data_loader:\n",
    "    json = paragraphs_json[image_ids[ids[0]]]\n",
    "    DisplayImage(json['url'])\n",
    "    print(json['url'])\n",
    "    print(\"actual caption is: \")\n",
    "    for sentence in sent_detector.tokenize(json['paragraph']):\n",
    "        print(sentence.strip().lower())\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "        topics = topics.cuda()\n",
    "    features = encoder_cnn(images)\n",
    "    \n",
    "    results = decoder_rnn.sample(features, topics, beam_size)\n",
    "    #print(\"predicted captions for topic %s: \" %(topic_vocab(topics.data.select(0, 0))))\n",
    "    for result in results:\n",
    "        candidate = [word_vocab(i) for i in result[1][:-1]]\n",
    "        print(candidate)\n",
    "    random_topic = torch.LongTensor([random.randint(0, len(topic_vocab) - 1)])\n",
    "    if torch.cuda.is_available():\n",
    "        random_topic = random_topic.cuda()\n",
    "    print(\"testing changing topic to %s:\" %(topic_vocab(random_topic.data.select(0, 0))))\n",
    "    results = decoder_rnn.sample(features, random_topic, beam_size)\n",
    "    for result in results:\n",
    "        candidate = [word_vocab(i) for i in result[1][:-1]]\n",
    "        print(candidate)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.426414Z",
     "start_time": "2018-05-26T10:50:43.783Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/outputs/{}/train_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(output_train_data, f)\n",
    "with open(\"data/outputs/{}/val_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(output_val_data, f)\n",
    "with open(\"data/outputs/{}/bleu_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(bleu_score_data, f)\n",
    "with open(\"data/outputs/{}/rouge_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(rouge_score_data, f)\n",
    "with open(\"data/outputs/{}/cider_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(cider_score_data, f)\n",
    "with open(\"data/outputs/{}/attention_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(attention_val_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Custom Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.427353Z",
     "start_time": "2018-05-26T10:50:44.015Z"
    }
   },
   "outputs": [],
   "source": [
    "image_name = \"snow\"\n",
    "topic = torch.LongTensor([10])\n",
    "image = Image.open(\"../data/custom/\" + image_name + \".jpg\").convert('RGB')\n",
    "images = [transform(image)]\n",
    "images = torch.stack(images, 0)\n",
    "if torch.cuda.is_available():\n",
    "    images = images.cuda()\n",
    "    topics = topics.cuda()\n",
    "    \n",
    "features = encoder_cnn(images)\n",
    "results = decoder_rnn.sample(features, topics, 10)\n",
    "print(\"predicted captions for topic %s: \" %(topic_vocab(topics.data.select(0, 0))))\n",
    "for result in results:\n",
    "    candidate = [word_vocab(i) for i in result[1][:-1]]\n",
    "    print(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.428307Z",
     "start_time": "2018-05-26T10:50:44.250Z"
    }
   },
   "outputs": [],
   "source": [
    "print(image_ids[(dev_data[3][0])])\n",
    "print(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.429327Z",
     "start_time": "2018-05-26T10:50:44.257Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "image_id = 2331872\n",
    "# 2346802 is a snow image\n",
    "epoch = 2\n",
    "alpha_values = attention_val_data[(epoch, image_id)]\n",
    "max_alpha = max(alpha_values)\n",
    "alpha_values = [alpha / max_alpha for alpha in alpha_values]\n",
    "\n",
    "original_image = Image.open(\"../data/\" + str(image_id) + \".jpg\") \\\n",
    "    .convert('L').resize((256, 256), Image.ANTIALIAS).crop((0, 0, 224, 224))\n",
    "\n",
    "image = Image.open(\"../data/\" + str(image_id) + \".jpg\").convert('L')\n",
    "image = image.resize((256, 256), Image.ANTIALIAS)\n",
    "image = image.crop((0, 0, 224, 224))\n",
    "pixels = image.load()\n",
    "row_pixels = image.size[0] // 14\n",
    "col_pixels = image.size[1] // 14\n",
    "for i in range(image.size[0]):    # for every col:\n",
    "    for j in range(image.size[1]):    # For every row\n",
    "        alpha = alpha_values[(i // row_pixels) * 14 + j // col_pixels]\n",
    "        pixels[i, j] = int(alpha * pixels[i, j])\n",
    "result = Image.blend(image, original_image, 0.2)\n",
    "result = result.resize((448, 448), Image.ANTIALIAS)\n",
    "display(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "993px",
    "right": "20px",
    "top": "170px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
