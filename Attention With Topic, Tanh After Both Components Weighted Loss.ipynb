{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Packages and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:27:14.837338Z",
     "start_time": "2018-05-27T08:27:13.324753Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from IPython.display import Image as DisplayImage\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import spacy\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# GPU Selection\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:27:22.105153Z",
     "start_time": "2018-05-27T08:27:18.879846Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/raw/splits/train_split.json\", \"r\") as f:\n",
    "    train_split = json.load(f)\n",
    "with open(\"data/raw/splits/dev_split.json\", \"r\") as f:\n",
    "    dev_split = json.load(f)\n",
    "with open(\"data/raw/splits/test_split.json\", \"r\") as f:\n",
    "    test_split = json.load(f)\n",
    "\n",
    "with open(\"data/raw/paragraphs_topics_v1.pickle\", \"rb\") as f:\n",
    "    paragraph_topics = pickle.load(f)\n",
    "with open(\"data/raw/paragraphs_v1.json\", \"r\") as f:\n",
    "    paragraphs_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:38:09.576304Z",
     "start_time": "2018-05-27T08:27:37.327951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d64533ce0547688ec449d0d2a892b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19561), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of train split: 269156\n",
      "Length of dev split: 47031\n",
      "Length of test split: 46013\n"
     ]
    }
   ],
   "source": [
    "def parse_data(paragraphs_json, progress_bar=True):\n",
    "    train_data = []\n",
    "    dev_data = []\n",
    "    test_data = []\n",
    "    image_ids = {}\n",
    "    topic_set = set()\n",
    "    for i, json in enumerate(tqdm_notebook(paragraphs_json) if progress_bar else paragraphs_json):\n",
    "        topic_to_seq = {}\n",
    "        for j, sentence in enumerate(sent_detector.tokenize(json['paragraph'])):\n",
    "            sentence = sentence.strip().lower()\n",
    "            t = nlp(sentence)\n",
    "            image_id = json['image_id']\n",
    "            image_ids[image_id] = i\n",
    "\n",
    "            if 'perfect_match' in paragraph_topics[i][j]:\n",
    "                topic_list = set([\n",
    "                    topic[0]\n",
    "                    for topic in paragraph_topics[i][j]['perfect_match']\n",
    "                ])\n",
    "                for topic in topic_list:\n",
    "                    topic_set.add(topic)\n",
    "                    if topic not in topic_to_seq:\n",
    "                        topic_to_seq[topic] = []\n",
    "                    topic_to_seq[topic].extend(t)\n",
    "            for topic in topic_to_seq:\n",
    "                if image_id in train_split:\n",
    "                    train_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "                elif image_id in dev_split:\n",
    "                    dev_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "                elif image_id in test_split:\n",
    "                    test_data.append((image_id, topic, topic_to_seq[topic]))\n",
    "    return train_data, dev_data, test_data, image_ids, topic_set\n",
    "\n",
    "train_data, dev_data, test_data, image_ids, topic_set = parse_data(paragraphs_json)\n",
    "print(\"Length of train split: %d\" % len(train_data))\n",
    "print(\"Length of dev split: %d\" % len(dev_data))\n",
    "print(\"Length of test split: %d\" % len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:38:59.317630Z",
     "start_time": "2018-05-27T08:38:59.312787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'url': u'https://cs.stanford.edu/people/rak248/VG_100K/2317429.jpg', u'image_id': 2317429, u'paragraph': u'A white round plate is on a table with a plastic tablecloth on it.  Two foil covered food halves are on the white plate along with a serving of golden yellow french fries.  Next to the white plate in a short,  topless, plastic container is a white sauce.  Diagonal to the white plate are the edges of several other stacked plates.  There are black shadows reflected on the table.'}\n",
      "1\n",
      "(2317429, 'color', [a, white, round, plate, is, on, a, table, with, a, plastic, tablecloth, on, it, ., two, foil, covered, food, halves, are, on, the, white, plate, along, with, a, serving, of, golden, yellow, french, fries, ., next, to, the, white, plate, in, a, short, ,,  , topless, ,, plastic, container, is, a, white, sauce, ., diagonal, to, the, white, plate, are, the, edges, of, several, other, stacked, plates, ., there, are, black, shadows, reflected, on, the, table, .])\n",
      "set(['body', 'emotion', 'transportation', 'people', 'color', 'signage', 'food', 'setting', 'animal', 'activity', 'weather', 'clothing'])\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs_json[1])\n",
    "print(image_ids[(train_data[3][0])])\n",
    "print(train_data[3])\n",
    "print(topic_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:39:59.249617Z",
     "start_time": "2018-05-27T08:39:53.037607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d966bf2f954474a28d03e9a8d3c00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=269156), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of word vocab: 10035\n",
      "Number of topics: 12\n",
      "set(['body', 'emotion', 'transportation', 'people', 'color', 'signage', 'food', 'setting', 'animal', 'activity', 'weather', 'clothing'])\n"
     ]
    }
   ],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"A vocabulary wrapper, contains a word_to_index dictionary and a index_to_word list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = []\n",
    "        self.index = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word_to_index:\n",
    "            self.word_to_index[word] = self.index\n",
    "            self.index_to_word.append(word)\n",
    "            self.index += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if type(word) == int:\n",
    "            return self.index_to_word[word]\n",
    "        else:\n",
    "            if not word in self.word_to_index:\n",
    "                return self.word_to_index['<UNK>']\n",
    "            return self.word_to_index[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.index\n",
    "\n",
    "\n",
    "def build_vocab(sentences, min_occurrences):\n",
    "    \"\"\"Builds a Vocabulary object\"\"\"\n",
    "    counter = Counter()\n",
    "    for sentence in tqdm_notebook(sentences):\n",
    "        for word in sentence:\n",
    "            counter[word.text] += 1\n",
    "\n",
    "    # a word must appear at least min_occurrence times to be included in the vocabulary\n",
    "    words = [\n",
    "        word for word, count in counter.items() if count >= min_occurrences\n",
    "    ]\n",
    "\n",
    "    # Creating a vocabulary object\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<SOS>')\n",
    "    vocab.add_word('<EOS>')\n",
    "    vocab.add_word('<UNK>')\n",
    "\n",
    "    # Adds the words from the captions to the vocabulary\n",
    "    for word in words:\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "word_vocab = build_vocab([val[2] for val in train_data], 5)\n",
    "topic_vocab = Vocabulary()\n",
    "for topic in topic_set:\n",
    "    topic_vocab.add_word(topic)\n",
    "print(\"Length of word vocab: %d\" % len(word_vocab))\n",
    "print(\"Number of topics: %d\" % len(topic_vocab))\n",
    "print(topic_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the Data\n",
    "The data is batched so sentences of the same length are grouped together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:40:32.066328Z",
     "start_time": "2018-05-27T08:40:24.808963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train batches: 16743\n",
      "number of dev batches: 2869\n",
      "number of test batches: 46013\n"
     ]
    }
   ],
   "source": [
    "class BatchedData(object):\n",
    "    def __init__(self, batch_size):\n",
    "        self.batched_data = []\n",
    "        self.index = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add_batch(self, batch):\n",
    "        if len(batch) == self.batch_size:\n",
    "            self.batched_data.append(batch)\n",
    "        else:\n",
    "            print(\"not the correct size batch!\")\n",
    "\n",
    "    def __call__(self, index):\n",
    "        if not index < len(self.batched_data):\n",
    "            return []\n",
    "        return self.batched_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batched_data)\n",
    "\n",
    "def batch_data(data, batch_size, progress_bar=True, randomize=True, max_size=None):\n",
    "    batched_set = {}\n",
    "    counter = 0\n",
    "    for (image_id, topic, sentence) in (tqdm_notebook(data) if progress_bar else data):\n",
    "        # accounting for SOS and EOS tokens\n",
    "        sentence = [token.text for token in sentence]\n",
    "        caption_len = len(sentence) + 2\n",
    "        if caption_len not in batched_set.keys():\n",
    "            batched_set[caption_len] = []\n",
    "        batched_set[caption_len].append((image_id, topic, [token for token in sentence]))\n",
    "\n",
    "    batched_data = BatchedData(batch_size)\n",
    "\n",
    "    curr_size = 0\n",
    "    for i in batched_set.keys():\n",
    "        if len(batched_set[i]) >= batch_size:\n",
    "            batch = batched_set[i]\n",
    "            if randomize:\n",
    "                random.shuffle(batch)\n",
    "            for j in range(len(batch) // batch_size):\n",
    "                if max_size is not None and curr_size == max_size:\n",
    "                    return batched_data\n",
    "                batched_data.add_batch(batch[batch_size * j:batch_size * (j + 1)])\n",
    "                curr_size += 1\n",
    "    if randomize:\n",
    "        random.shuffle(batched_data.batched_data)\n",
    "    return batched_data\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "batched_train_data = batch_data(train_data, batch_size, progress_bar=False)\n",
    "batched_dev_data = batch_data(dev_data, batch_size, progress_bar=False)\n",
    "batched_single_dev_data = batch_data(dev_data, 1, progress_bar=False, randomize=False)\n",
    "batched_test_data = batch_data(test_data, 1, progress_bar=False, randomize=False)\n",
    "\n",
    "print(\"number of train batches: %d\" % len(batched_train_data))\n",
    "print(\"number of dev batches: %d\" % len(batched_dev_data))\n",
    "print(\"number of test batches: %d\" % len(batched_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transform\n",
    "The transformation that will be applied to all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:41:00.766043Z",
     "start_time": "2018-05-27T08:41:00.757624Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:41:27.719964Z",
     "start_time": "2018-05-27T08:41:27.686009Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataSet(data.Dataset):\n",
    "    def __init__(self, paragraphs,  batched_captions, word_vocab, topic_vocab, transform=None):\n",
    "        \"\"\"\n",
    "        Set the path for images, captions and vocabulary wrapper.\n",
    "    \n",
    "        Args:\n",
    "        \n",
    "                vocab: vocabulary wrapper.\n",
    "                transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.paragraphs = paragraphs\n",
    "        self.batched_captions = batched_captions\n",
    "        self.word_vocab = word_vocab\n",
    "        self.topic_vocab = topic_vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        images = []\n",
    "        topics = []\n",
    "        captions = []\n",
    "        img_ids = []\n",
    "        for (image_id, topic, sentence) in self.batched_captions(index):\n",
    "            image = Image.open(\"data/images/%d.jpg\" % image_id).convert('RGB')\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "            images.append(image)\n",
    "            topics.append(self.topic_vocab(topic))\n",
    "            img_ids.append(image_id)\n",
    "            captions.append([self.word_vocab('<SOS>')] +\n",
    "                            [self.word_vocab(token) for token in sentence] +\n",
    "                            [self.word_vocab('<EOS>')])\n",
    "\n",
    "        lengths = [len(caption) for caption in captions]\n",
    "        return torch.stack(images, 0), torch.LongTensor(\n",
    "            topics), torch.LongTensor(captions), lengths, img_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batched_captions)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "  \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging captions (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    images, topics, captions, lengths, image_ids = zip(*data)\n",
    "    return images[0], topics[0], captions[0], lengths[0], image_ids[0]\n",
    "\n",
    "\n",
    "def get_loader(paragraphs, batched_data, word_vocab, topic_vocab, transform,\n",
    "               shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    data_set = CustomDataSet(paragraphs, batched_data, word_vocab, topic_vocab,\n",
    "                             transform)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=data_set,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn)\n",
    "    return data_loader\n",
    "\n",
    "test_data_loader = get_loader(test_data, batched_test_data, word_vocab,\n",
    "                              topic_vocab, transform, False, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model\n",
    "## The Encoder\n",
    "The encoder is a CNN, specifically VGG-16 with which we strip off the final few layers to get 196 visual feature vectors of size 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:41:55.343361Z",
     "start_time": "2018-05-27T08:41:54.186048Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        vgg = models.vgg16(pretrained=True).eval()\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = nn.Sequential(*(vgg.features[i] for i in range(29)))\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.vgg(images)\n",
    "        features_reshaped = features.view(-1, 512, 196)\n",
    "        features_transposed = features_reshaped.transpose(1, 2)\n",
    "        return features_transposed\n",
    "\n",
    "\n",
    "encoder_cnn = EncoderCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder\n",
    "The decoder is an LSTM with attention and dropout. We compute the initial hidden state and cell state as follows:\n",
    "$$h_{0} = \\tanh((\\frac{1}{196} \\sum_{i=0}^{196} f_{initH}(a_{i})) + g_{initH}(t_{j}))$$\n",
    "$$c_{0} = \\tanh((\\frac{1}{196} \\sum_{i=0}^{196} f_{initC}(a_{i})) + g_{initC}(t_{j}))$$\n",
    "where $f_{initH}, f_{initC}, g_{initH}, g_{initC}$ are all linear functions, $a_{i}$ is the $i^{th}$ annotation vector and $t_{j}$ is the topic embedding.\n",
    "We compute attention using the soft attention model from Show Tell Attend as follows:\n",
    "$$e_{ti} = w_{1} a_{i} + w_{2} h_{t-1} + w_{3} t_{j}$$\n",
    "$$\\alpha_{ti} = \\frac{\\exp(e_{ti})}{\\sum_{k=1}^{L} \\exp(e_{tk})}$$\n",
    "and the context vector $\\hat{z_{t}} = \\sum_{i=1}^{L} \\alpha_{ti} a_{i}$,\n",
    "where $w_{1}, w_{2}, w_{3}$ are all learned matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:42:22.293134Z",
     "start_time": "2018-05-27T08:42:21.984655Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_predict_input_captions(captions):\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.LongTensor(captions)\n",
    "    return torch.LongTensor(captions)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vis_dim: The size of each visual feature vector\n",
    "        vis_num: The number of visual feature vectors\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vis_dim, vis_num, embed_dim, hidden_dim, \n",
    "                 word_vocab_size, topic_vocab_size, num_layers=1, dropout=0.0):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.vis_dim = vis_dim\n",
    "        self.vis_num = vis_num\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        self.topic_vocab_size = topic_vocab_size\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.init_h_vis = nn.Linear(vis_dim, hidden_dim, bias=False)\n",
    "        self.init_c_vis = nn.Linear(vis_dim, hidden_dim, bias=False)\n",
    "        self.init_h_topic = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.init_c_topics = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.attn_vw = nn.Linear(vis_dim, 1)\n",
    "        self.attn_hw = nn.Linear(hidden_dim, 1)\n",
    "        self.attn_tw = nn.Linear(embed_dim, 1)\n",
    "\n",
    "        self.topic_embed = nn.Embedding(topic_vocab_size, embed_dim)\n",
    "        self.word_embed = nn.Embedding(word_vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(vis_dim + embed_dim, hidden_dim, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_dim, word_vocab_size)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "\n",
    "    def _init_hidden(self, features, topics):\n",
    "        tanh = nn.Tanh()\n",
    "        hidden = ((tanh(torch.sum(self.init_h_vis(features), 1) / self.vis_num)) + self.init_h_topic(topics))\n",
    "        cell = ((tanh(torch.sum(self.init_c_vis(features), 1) / self.vis_num)) + self.init_c_topics(topics))\n",
    "        return hidden.unsqueeze(0), cell.unsqueeze(0)\n",
    "\n",
    "    def _compute_attention(self, features, topics, hidden_state):\n",
    "        \"\"\"\n",
    "        features: B x vis_num x vis_dim\n",
    "        hidden_state: (1 x B x hidden_size, 1 x B x hidden_size)\n",
    "        \"\"\"\n",
    "        # add in L1 norm (sum up everything and divide everything by sum\n",
    "        #features = torch.norm(features, 1, 2, )\n",
    "        # B x vis_num x 1\n",
    "        att_vw = self.attn_vw(features)\n",
    "        # B x vis_num x 1\n",
    "        att_hw = self.attn_hw(\n",
    "            hidden_state.transpose(0, 1).repeat(1, self.vis_num, 1))\n",
    "        att_tw = self.attn_tw(topics).unsqueeze(1).repeat(1, self.vis_num, 1)\n",
    "        # B x vis_num x 1\n",
    "        attention = att_vw + att_hw + att_tw\n",
    "        attention_softmax = F.softmax(attention, dim=1)\n",
    "        # B x vis_dim\n",
    "        # also return the attention_softmax and average those\n",
    "        return torch.sum(features * attention_softmax, 1), attention_softmax\n",
    "\n",
    "    def forward(self, features, topics, captions):\n",
    "        \"\"\"\n",
    "        topic: B x 1\n",
    "        features: B x vis_num x vis_dim\n",
    "        captions: B x seq_length\n",
    "        \"\"\"\n",
    "        topic_embeddings = self.topic_embed(topics)\n",
    "        hidden = self._init_hidden(features, topic_embeddings)\n",
    "        word_embeddings = self.word_embed(captions)\n",
    "        word_space = None\n",
    "        lengths = len(captions[0])\n",
    "        average_attention = None\n",
    "        for i in range(lengths):\n",
    "            embedding = torch.index_select(\n",
    "                word_embeddings, 1, torch.cuda.LongTensor([i]))\n",
    "            attention, alpha = self._compute_attention(features, topic_embeddings, hidden[0])\n",
    "            attention = attention.unsqueeze(1)\n",
    "            #average_attention = alpha if average_attention is None else average_attention + alpha\n",
    "            input = self.dropout_layer(torch.cat([attention, embedding], 2))\n",
    "            out, hidden = self.lstm(input, hidden)\n",
    "            words = self.output(self.dropout_layer(out))\n",
    "            word_space = torch.cat([word_space, words], 1) if word_space is not None else words\n",
    "        word_space = pack_padded_sequence(\n",
    "            word_space, [lengths for i in range(len(captions))],\n",
    "            batch_first=True)[0]\n",
    "        return F.log_softmax(\n",
    "            word_space, dim=1), F.softmax(\n",
    "                word_space, dim=1), #average_attention\n",
    "\n",
    "    def sample(self, features, topics, beam_size=1, start_token=0, end_token=1):\n",
    "        topic_embeddings = self.topic_embed(topics)\n",
    "        hidden = self._init_hidden(features, topic_embeddings)\n",
    "        completed_phrases = []\n",
    "        best_phrases = []\n",
    "        score = 0\n",
    "\n",
    "        initial_caption = create_predict_input_captions([start_token])\n",
    "        embedding = self.word_embed(initial_caption)\n",
    "        attention, _ = self._compute_attention(features, topic_embeddings,\n",
    "                                               hidden[0])\n",
    "        input = torch.cat([attention, embedding], 1).unsqueeze(1)\n",
    "        out, hidden = self.lstm(input, hidden)\n",
    "        words = self.output(out)\n",
    "        word_scores = F.softmax(words, dim=2)\n",
    "        top_scores, top_captions = word_scores.topk(beam_size)\n",
    "        best_phrases = [[\n",
    "            top_scores[0][0].data[i], [top_captions[0][0].data[i]]\n",
    "        ] for i in range(beam_size)]\n",
    "        next_captions = top_captions.resize(beam_size, 1)\n",
    "        hidden = (hidden[0].repeat(1, beam_size, 1), hidden[1].repeat(\n",
    "            1, beam_size, 1))\n",
    "\n",
    "        for index in range(40):\n",
    "            best_candidates = []\n",
    "            embedding = self.word_embed(next_captions)\n",
    "            attention, _ = self._compute_attention(features, topic_embeddings,\n",
    "                                                   hidden[0])\n",
    "            attention = attention.unsqueeze(1)\n",
    "            input = torch.cat([attention, embedding], 2)\n",
    "            out, hidden = self.lstm(input, hidden)\n",
    "            words = self.output(out)\n",
    "            word_scores = F.softmax(words, dim=2)\n",
    "            top_scores, top_captions = word_scores.topk(beam_size)\n",
    "            len_phrases = len(best_phrases[0][1])\n",
    "            for i in range(len(best_phrases)):\n",
    "                for j in range(beam_size):\n",
    "                    best_candidates.extend([[\n",
    "                        best_phrases[i][0] + top_scores[i][0].data[j],\n",
    "                        best_phrases[i][1] + [top_captions[i][0].data[j]], i\n",
    "                    ]])\n",
    "            top_candidates = sorted(\n",
    "                best_candidates,\n",
    "                key=lambda score_caption: score_caption[0])[-beam_size:]\n",
    "            temp_candidates = []\n",
    "            for phrase in top_candidates:\n",
    "                if phrase[1][-1] == end_token:\n",
    "                    completed_phrases.append(\n",
    "                        [phrase[0] / len(phrase[1]), phrase[1]])\n",
    "                else:\n",
    "                    temp_candidates.append(phrase)\n",
    "            top_candidates = temp_candidates\n",
    "            if len(completed_phrases) >= beam_size:\n",
    "                return sorted(\n",
    "                    completed_phrases,\n",
    "                    key=lambda score_caption: score_caption[0],\n",
    "                    reverse=True)[:beam_size]\n",
    "            best_phrases = [[phrase[0], phrase[1]]\n",
    "                            for phrase in top_candidates]\n",
    "            next_captions = create_predict_input_captions(\n",
    "                [[phrase[1][-1]] for phrase in top_candidates])\n",
    "            hidden_0 = (torch.stack([\n",
    "                hidden[0][0].select(0, phrase[2]) for phrase in top_candidates\n",
    "            ]).unsqueeze(0))\n",
    "            hidden_1 = (torch.stack([\n",
    "                hidden[1][0].select(0, phrase[2]) for phrase in top_candidates\n",
    "            ]).unsqueeze(0))\n",
    "            hidden = (hidden_0, hidden_1)\n",
    "        return sorted(\n",
    "            completed_phrases,\n",
    "            key=lambda score_caption: score_caption[0],\n",
    "            reverse=True)[:beam_size] if len(completed_phrases) != 0 else top_candidates\n",
    "\n",
    "\n",
    "decoder_rnn = DecoderRNN(512, 196, 512, 512, len(word_vocab), len(topic_vocab), num_layers=2, dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:42:53.962801Z",
     "start_time": "2018-05-27T08:42:49.071955Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    encoder_cnn.cuda()\n",
    "    decoder_rnn.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Loss Function and  Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfaa13c69ad040c68effe59b18aaff5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=269156), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_train_data = batch_data(train_data, 1, progress_bar=False, randomize=False)\n",
    "single_train_data_loader = get_loader(train_data, single_train_data, word_vocab,\n",
    "                              topic_vocab, transform, False, 2)\n",
    "counter = Counter()\n",
    "for images, topics, captions, lengths, image_ids in tqdm_notebook(single_train_data_loader):\n",
    "    counter.update([x.item() for x in captions[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T08:43:20.657449Z",
     "start_time": "2018-05-27T08:43:20.650488Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "counts = [count for _, count in sorted(counter.items(), key=lambda item: item[0])]\n",
    "num_words = 0\n",
    "for count in counts:\n",
    "    num_words += count\n",
    "weight = torch.FloatTensor([1 / (1 + exp(1.0 * count / num_words)) for count in counts])\n",
    "if torch.cuda.is_available():\n",
    "    weight = weight.cuda()\n",
    "loss_function = nn.NLLLoss(weight=weight)\n",
    "params = list(decoder_rnn.parameters())\n",
    "optimizer = optim.Adam(params, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Saved Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:35.881641Z",
     "start_time": "2018-05-26T10:51:35.875463Z"
    }
   },
   "outputs": [],
   "source": [
    "load_checkpoint = None\n",
    "if load_checkpoint is not None:\n",
    "    checkpoint = torch.load(load_checkpoint)\n",
    "    print(\"loading from checkpoint \" + str(load_checkpoint))\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    decoder_rnn.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    del checkpoint\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:35.912659Z",
     "start_time": "2018-05-26T10:51:35.885187Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(images, topics, captions, encoder_cnn, decoder_rnn, loss_function):\n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()     \n",
    "        topics = topics.cuda()\n",
    "        captions = captions.cuda()\n",
    "        \n",
    "    inputs = captions[:, :-1]\n",
    "    targets = captions[:, 1:]   \n",
    "    features = encoder_cnn(images)\n",
    "    len_targets = len(targets[0])\n",
    "    targets = pack_padded_sequence(\n",
    "        targets, [len_targets for i in range(len(captions))],\n",
    "        batch_first=True)[0]\n",
    "    predictions, _ = decoder_rnn(features, topics, inputs)\n",
    "    loss = loss_function(predictions, targets)\n",
    "    return loss#, average_attention\n",
    "\n",
    "\n",
    "def train(images, topics, captions, encoder_cnn, decoder_rnn, loss_function,\n",
    "          optimizer, grad_clip):\n",
    "    decoder_rnn.train()\n",
    "    decoder_rnn.zero_grad()\n",
    "    # added in requires grad\n",
    "    loss = evaluate(images, topics.requires_grad_(), captions.requires_grad_(), encoder_cnn, decoder_rnn, loss_function)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(decoder_rnn.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "def val(images, topics, captions, encoder_cnn, decoder_rnn, loss_function):\n",
    "    decoder_rnn.eval()\n",
    "    loss = evaluate(images, topics, captions, encoder_cnn, decoder_rnn, loss_function)\n",
    "    return loss.data.item()#, average_attention.squeeze(0).squeeze(1).data\n",
    "\n",
    "\n",
    "def caption_id_to_string(caption, vocab):\n",
    "    output = \"\"\n",
    "    for word in caption:\n",
    "        if \".\" != word and \"<EOS>\" != word:\n",
    "            output += vocab(word) + \" \"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:35.925667Z",
     "start_time": "2018-05-26T10:51:35.916184Z"
    }
   },
   "outputs": [],
   "source": [
    "basedir = \"Attention With Topic, Tanh After Both Components Weighted Loss\"\n",
    "output_train_data = {}\n",
    "output_val_data = {}\n",
    "bleu_score_data = {}\n",
    "rouge_score_data = {}\n",
    "cider_score_data = {}\n",
    "attention_val_data = {}\n",
    "\n",
    "if load_checkpoint is not None:\n",
    "    with open(\"data/outputs/{}/train_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        output_train_data = pickle.load(f)\n",
    "    with open(\"data/outputs/{}/val_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        output_val_data = pickle.load(f)\n",
    "    with open(\"data/outputs/{}/bleu_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        bleu_score_data = pickle.load(f)\n",
    "    with open(\"data/outputs/{}/rouge_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        rouge_score_data = pickle.load(f)\n",
    "    with open(\"data/outputs/{}/cider_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        cider_score_data = pickle.load(f)\n",
    "    with open(\"data/outputs/{}/attention_{}.pkl\".format(basedir, start_epoch), \"rb\") as f:\n",
    "        attention_val_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:35.935199Z",
     "start_time": "2018-05-26T10:51:35.928771Z"
    }
   },
   "outputs": [],
   "source": [
    "scorer_bleu = Bleu(4)\n",
    "scorer_rouge = Rouge()\n",
    "scorer_cider = Cider()\n",
    "\n",
    "# Initialize dictionary for reference sequences and generated sequences\n",
    "# This is just the format these need to be in to be passed to the script\n",
    "sequences_ref = {}\n",
    "sequences_gen = {}\n",
    "\n",
    "# Define words that you don't want to count as correct\n",
    "# Things like start tokens, end tokens, <unk> tokens, <pad> tokens, etc.\n",
    "# Up to you what these are, though\n",
    "bad_words = ['<SOS>', '<EOS>', '<UNK>']\n",
    "\n",
    "# Convert list of tokens to list of token_indices in vocab\n",
    "bad_toks = [word_vocab(i) for i in bad_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.421693Z",
     "start_time": "2018-05-26T10:51:35.937369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0ec79409084adaaf22b29162b0ab46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Train [1/10]', max=16743), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fc0b7346390>> ignored\n",
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fc1bbda9090>> ignored\n",
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fc0b7346390>> ignored\n",
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fc0b7346390>> ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8786fa7bb80f4feca1d429faea005c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Val [2/10]', max=2869), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a945fe99b13c45a4ad7e19b3910376a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Train [3/10]', max=16743), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fc1d1afb510>> ignored\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba43d488ffe4e33b26bb9e9b47fcc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Train [4/10]', max=16743), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fc103f12250>> ignored\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "num_epochs = 10\n",
    "grad_clip = 5.0\n",
    "beam_size=10\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # Reshuffle data\n",
    "    batched_train_data = batch_data(train_data, batch_size, progress_bar=False)\n",
    "    batched_dev_data = batch_data(dev_data, batch_size, progress_bar=False)\n",
    "    \n",
    "    train_data_loader = get_loader(train_data, batched_train_data, word_vocab, topic_vocab, transform, shuffle=True, num_workers=2)\n",
    "    temp_val_loader = get_loader(dev_data, batched_dev_data, word_vocab, topic_vocab, transform, shuffle=True, num_workers=2)\n",
    "\n",
    "    progress_bar = tqdm_notebook(iterable=train_data_loader, desc='Train [%i/%i]' %(epoch + 1 , num_epochs))\n",
    "    train_sum_loss = 0.0\n",
    "    \n",
    "    for i, (images, topics, captions, lengths, ids) in enumerate(progress_bar, 1):\n",
    "        train_sum_loss += train(images, topics, captions, encoder_cnn, decoder_rnn, loss_function, optimizer, grad_clip)\n",
    "        del images, topics, captions, lengths\n",
    "        torch.cuda.empty_cache()\n",
    "        progress_bar.set_postfix(loss=train_sum_loss/(i % 100 if i % 100 != 0 else 1))\n",
    "        if i % 100 == 0:\n",
    "            output_train_data[epoch * len(train_data_loader) + i] = train_sum_loss / 100\n",
    "            train_sum_loss = 0\n",
    "        if i % 2000 == 0:\n",
    "            temp_val_loss = 0.0\n",
    "            for j, (images, topics, captions, lengths, ids) in enumerate(temp_val_loader):\n",
    "                if j == 100:\n",
    "                    break\n",
    "                temp_val_loss += val(images, topics, captions, encoder_cnn, decoder_rnn, loss_function)\n",
    "                del images, topics, captions, lengths\n",
    "                torch.cuda.empty_cache()\n",
    "            output_val_data[epoch * len(train_data_loader) + i] = temp_val_loss / 100\n",
    "    \n",
    "    # end of batch\n",
    "    if len(train_data_loader) % 100 != 0:\n",
    "        output_train_data[(epoch + 1) * len(train_data_loader)] = train_sum_loss / (len(train_data_loader) % 100)\n",
    "    \n",
    "    # validation loss\n",
    "    val_sum_loss = 0\n",
    "    val_data_loader = get_loader(dev_data, batched_dev_data, word_vocab, topic_vocab, transform, shuffle=False, num_workers=2)\n",
    "    val_progress_bar = tqdm_notebook(iterable=val_data_loader, desc='Val [%i/%i]' %(epoch + 1, num_epochs))\n",
    "    for i, (images, topics, captions, lengths, ids) in enumerate(val_progress_bar, 1):\n",
    "        loss = val(images, topics, captions, encoder_cnn, decoder_rnn, loss_function)\n",
    "        # attention_val_data[(epoch + 1, ids[0])] = average_attention\n",
    "        val_sum_loss += loss\n",
    "        val_progress_bar.set_postfix(loss=val_sum_loss/i)\n",
    "        del images, topics, captions, lengths\n",
    "        torch.cuda.empty_cache()\n",
    "    output_val_data[(epoch + 1) * len(train_data_loader)] = val_sum_loss / len(val_data_loader)\n",
    "    \n",
    "    torch.save({'epoch': epoch + 1,\n",
    "            'state_dict': decoder_rnn.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()},\n",
    "            \"data/checkpoints/{}/checkpoint_{}.pt\".format(basedir, epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5, 20, 5):\n",
    "    load_checkpoint = \"data/checkpoints/{}/checkpoint_{}.pt\".format(basedir, epoch)\n",
    "    checkpoint = torch.load(load_checkpoint)\n",
    "    print(\"loading from checkpoint \" + str(load_checkpoint))\n",
    "    decoder_rnn.load_state_dict(checkpoint['state_dict'])\n",
    "    del checkpoint\n",
    "    torch.cuda.empty_cache()\n",
    "    # generating captions\n",
    "\n",
    "    batched_single_dev_data = batch_data(dev_data, 1, progress_bar=False, randomize=False)\n",
    "    single_val_data_loader = get_loader(dev_data, batched_single_dev_data,\n",
    "                                        word_vocab, topic_vocab, transform, False, 2)\n",
    "    generate_progress_bar = tqdm_notebook(iterable=single_val_data_loader)\n",
    "    for i, (images, topics, captions, lengths, ids) in enumerate(generate_progress_bar, 1):\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            topics = topics.cuda()\n",
    "        features = encoder_cnn(images)\n",
    "        results = decoder_rnn.sample(features, topics, beam_size)\n",
    "        sequences_ref[i] = [\" \".join([word_vocab(j.item()) for j in captions[0]\n",
    "                            if j not in bad_toks])]\n",
    "        sequences_gen[i] = [\" \".join([word_vocab(j.item()) for j in results[0][1]\n",
    "                            if j not in bad_toks])]\n",
    "        del images, topics, captions, lengths\n",
    "        torch.cuda.empty_cache()\n",
    "    bleu_score, bleu_scores = scorer_bleu.compute_score(\n",
    "        sequences_ref, sequences_gen)\n",
    "    bleu_score_data[epoch] = bleu_score\n",
    "    rouge_score, rouge_scores = scorer_rouge.compute_score(\n",
    "        sequences_ref, sequences_gen)\n",
    "    rouge_score_data[epoch] = rouge_score\n",
    "    cider_score, cider_scores = scorer_cider.compute_score(\n",
    "        sequences_ref, sequences_gen)\n",
    "    cider_score_data[epoch] = cider_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(bleu_score_data.keys()):\n",
    "    print(\"Bleu at epoch {}: {}\".format(key, bleu_score_data[key]))\n",
    "    print(\"Rouge at epoch {}: {}\".format(key, rouge_score_data[key]))\n",
    "    print(\"Cider at epoch {}: {}\".format(key, cider_score_data[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.423374Z",
     "start_time": "2018-05-26T10:50:43.061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc11cd4c810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_trial_number = sorted(output_train_data.keys())\n",
    "train_trial_loss = [output_train_data[i] for i in train_trial_number]\n",
    "val_trial_number = sorted(output_val_data.keys())\n",
    "val_trial_loss = [output_val_data[i] for i in val_trial_number]\n",
    "\n",
    "\n",
    "plt.plot(train_trial_number, train_trial_loss, 'ro', markersize=1, label='Training Batch Loss')\n",
    "plt.plot(val_trial_number, val_trial_loss, 'bo', markersize=5, label='Validation Loss After 1 Full Epoch')\n",
    "\n",
    "plt.xlabel('Number of Batches')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over ' + str(num_epochs) + ' Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.424442Z",
     "start_time": "2018-05-26T10:50:43.294Z"
    }
   },
   "outputs": [],
   "source": [
    "beam_size = 10\n",
    "batched_temp_data = batch_data(train_data, 1, progress_bar=False)\n",
    "temp_data_loader = get_loader(train_data, batched_temp_data, word_vocab, topic_vocab, transform, True, 2)\n",
    "for images, topics, captions, lengths, ids in temp_data_loader:\n",
    "    json = paragraphs_json[image_ids[ids[0]]]\n",
    "    DisplayImage(json['url'])\n",
    "    print(json['url'])\n",
    "    print(\"actual caption is: \")\n",
    "    for sentence in sent_detector.tokenize(json['paragraph']):\n",
    "        print(sentence.strip().lower())\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "        topics = topics.cuda()\n",
    "    features = encoder_cnn(images)\n",
    "    \n",
    "    results = decoder_rnn.sample(features, topics, beam_size)\n",
    "    print(\"predicted captions for topic %s: \" %(topic_vocab(topics.data.select(0, 0))))\n",
    "    for result in results:\n",
    "        candidate = [word_vocab(i) for i in result[1][:-1]]\n",
    "        print(candidate)\n",
    "    random_topic = torch.LongTensor([random.randint(0, len(topic_vocab) - 1)])\n",
    "    if torch.cuda.is_available():\n",
    "        random_topic = random_topic.cuda()\n",
    "    print(\"testing changing topic to %s:\" %(topic_vocab(random_topic.data.select(0, 0))))\n",
    "    results = decoder_rnn.sample(features, random_topic, beam_size)\n",
    "    for result in results:\n",
    "        candidate = [word_vocab(i) for i in result[1][:-1]]\n",
    "        print(candidate)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.426414Z",
     "start_time": "2018-05-26T10:50:43.783Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/outputs/{}/train_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(output_train_data, f)\n",
    "with open(\"data/outputs/{}/val_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(output_val_data, f)\n",
    "with open(\"data/outputs/{}/bleu_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(bleu_score_data, f)\n",
    "with open(\"data/outputs/{}/rouge_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(rouge_score_data, f)\n",
    "with open(\"data/outputs/{}/cider_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(cider_score_data, f)\n",
    "with open(\"data/outputs/{}/attention_{}.pkl\".format(basedir, num_epochs), \"wb\") as f:\n",
    "    pickle.dump(attention_val_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Custom Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.427353Z",
     "start_time": "2018-05-26T10:50:44.015Z"
    }
   },
   "outputs": [],
   "source": [
    "image_name = \"snow\"\n",
    "topic = torch.LongTensor([10])\n",
    "image = Image.open(\"../data/custom/\" + image_name + \".jpg\").convert('RGB')\n",
    "images = [transform(image)]\n",
    "images = torch.stack(images, 0)\n",
    "if torch.cuda.is_available():\n",
    "    images = images.cuda()\n",
    "    topics = topics.cuda()\n",
    "    \n",
    "features = encoder_cnn(images)\n",
    "results = decoder_rnn.sample(features, topics, 10)\n",
    "print(\"predicted captions for topic %s: \" %(topic_vocab(topics.data.select(0, 0))))\n",
    "for result in results:\n",
    "    candidate = [word_vocab(i) for i in result[1][:-1]]\n",
    "    print(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.428307Z",
     "start_time": "2018-05-26T10:50:44.250Z"
    }
   },
   "outputs": [],
   "source": [
    "print(image_ids[(dev_data[3][0])])\n",
    "print(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T10:51:36.429327Z",
     "start_time": "2018-05-26T10:50:44.257Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "image_id = 2331872\n",
    "# 2346802 is a snow image\n",
    "epoch = 2\n",
    "alpha_values = attention_val_data[(epoch, image_id)]\n",
    "max_alpha = max(alpha_values)\n",
    "alpha_values = [alpha / max_alpha for alpha in alpha_values]\n",
    "\n",
    "original_image = Image.open(\"../data/\" + str(image_id) + \".jpg\") \\\n",
    "    .convert('L').resize((256, 256), Image.ANTIALIAS).crop((0, 0, 224, 224))\n",
    "\n",
    "image = Image.open(\"../data/\" + str(image_id) + \".jpg\").convert('L')\n",
    "image = image.resize((256, 256), Image.ANTIALIAS)\n",
    "image = image.crop((0, 0, 224, 224))\n",
    "pixels = image.load()\n",
    "row_pixels = image.size[0] // 14\n",
    "col_pixels = image.size[1] // 14\n",
    "for i in range(image.size[0]):    # for every col:\n",
    "    for j in range(image.size[1]):    # For every row\n",
    "        alpha = alpha_values[(i // row_pixels) * 14 + j // col_pixels]\n",
    "        pixels[i, j] = int(alpha * pixels[i, j])\n",
    "result = Image.blend(image, original_image, 0.2)\n",
    "result = result.resize((448, 448), Image.ANTIALIAS)\n",
    "display(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "993px",
    "right": "20px",
    "top": "170px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
